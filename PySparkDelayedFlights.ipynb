{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySparkDelayedFlights",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySparkDelayedFlights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook requires a large dataset be loaded onto your Google Drive because it is too large for the github cloned repo. "
      ],
      "metadata": {
        "id": "b2I7IfDA4x8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Go to this [kaggle link](https://www.kaggle.com/code/miquar/explore-flights-csv-airports-csv-airlines-csv/data?select=flights.csv)<br>\n",
        "\n",
        "2. Download the data by clicking on the Download button in the top right corner. \n",
        "\n",
        "3. This will download to your local computer a zip folder called archive.zip\n",
        "\n",
        "4. Open the folder by clicking on the archive folder\n",
        "\n",
        "5. Go to your Google drive and upload flights.csv by doing the following: \n",
        ">a. In the upper left corner click on \"New\"<br>\n",
        "b. Click on Upload file<br>\n",
        "c. On your local machine find the archive folder that you opened<br>\n",
        "d. Select the file called flights.csv<br>\n",
        " This is a large file (565MB) so it will take a few minutes to upload. \n",
        "\n",
        "Once you see the file in your Google Drive, you are ready to start working with this notebook.\n"
      ],
      "metadata": {
        "id": "hhMRj-BH6q20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount your Google Drive to this notebook**"
      ],
      "metadata": {
        "id": "iuZ2yq36Cw9A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ettu8VG7-KAI"
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed Computing<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "2i0Jm7Kj81iU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WcC7uUz7S4u"
      },
      "source": [
        "https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3\n",
        "\n",
        "\n",
        "https://medium.com/grabngoinfo/install-pyspark-3-on-google-colab-the-easy-way-577ec4a2bcd8\n",
        "\n",
        "Check for the latest version of Spark at https://pypi.org/project/pyspark/#history\n",
        "\n",
        "https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp\n",
        "\n",
        "\n",
        "https://grabngoinfo.com/install-pyspark-3-on-google-colab-the-easy-way/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cagBRT/PySpark.git"
      ],
      "metadata": {
        "id": "vn0ujiMgQXc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "gkVZEiasOkh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an Entry Point to PySpark\n",
        "<br>\n",
        "SparkSession starts the PySpark Session. \n",
        "<br>\n",
        "This begins the ability to program with RDD (Resilient Data Distribution), DataFrame, and DataSet"
      ],
      "metadata": {
        "id": "cq9uwbI57gzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "#getOrCreate gets or creates a session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "vvON8-UoOqQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "7ZLp5GP1OtxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD5KzNGA7RZH"
      },
      "source": [
        "import os\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "#import findspark\n",
        "#findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "print(\"If no error - everything is working\")\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to mount the Gdrive because the dataset is in three files and one of them is quite large. <bR>\n",
        "The cloned repo contains two of the files:<Br>\n",
        "\"/content/cloned-repo/airports.csv\"<br>\n",
        "\"/content/cloned-repo/airlines.csv\"<br>\n",
        "<br>\n",
        "The third file needs to be loaded onto your GDrive and then the program can read it from there. \n"
      ],
      "metadata": {
        "id": "y8DzAiKP3w7J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_SqXaIST-eQ"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "#%cd cloned-repo\n",
        "#!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k67iOVTXEfEg"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1RFC0UzFEX5"
      },
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up constants\n",
        "FLIGHTS= \"/gdrive/MyDrive/flights.csv\" \n",
        "AIRPORTS= \"/content/cloned-repo/airports.csv\"\n",
        "AIRLINES= \"/content/cloned-repo/airlines.csv\"\n",
        "APP_NAME = \"Flight Delays\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141109\n",
        "TRAINING_DATA_RATIO = 0.7\n",
        "RF_NUM_TREES = 8\n",
        "RF_MAX_DEPTH = 4\n",
        "RF_NUM_BINS = 32"
      ],
      "metadata": {
        "id": "IsOmfEz_FhRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Spark session has to be started. "
      ],
      "metadata": {
        "id": "vuvH93ewDCaB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0V8_n8LFJAb"
      },
      "source": [
        "# Connect to the Spark server\n",
        "spark = SparkSession.builder.appName(APP_NAME).master(FLIGHTS).getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is 5.8 milllion rows of data. <br>\n",
        "It will take a minute to load it into the Spark session"
      ],
      "metadata": {
        "id": "VyOpm6jVil0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "flights_df = spark.read.options(header=\"true\",inferschema = \"true\").csv(FLIGHTS)"
      ],
      "metadata": {
        "id": "CfOlY85HibQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next command can take a while to execute, be patient or skip it. "
      ],
      "metadata": {
        "id": "07mkZWHqEcbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#flights_df.describe().show()"
      ],
      "metadata": {
        "id": "mpAQ-bv4D9iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is 5.8million rows by 31 columns. <br>\n",
        "If your dataset is not 5819079 rows by 31 columns, you may not have uploaded the data properly. "
      ],
      "metadata": {
        "id": "kXjiWSnnidao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datacount=flights_df.groupBy(\"CANCELLED\").count()\n",
        "datacount.show()"
      ],
      "metadata": {
        "id": "jse-2FMxFjhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-raO-RCFzje"
      },
      "source": [
        "print(f\"The shape is {datacount.count():d} rows by {len(datacount.columns):d} columns.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has a lot of null values (missing values)<br>\n",
        "30 million missing values"
      ],
      "metadata": {
        "id": "tRcV9PGtizNm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj-kmimfGHoS"
      },
      "source": [
        "null_counts = datacount.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)for c in datacount.columns]).toPandas().to_dict(orient='records')\n",
        "print(f\"We have {sum(null_counts[0].values()):d} null values in this dataset.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list of columns and data types shows there are string value columns. If we are going to use this data we need to either change the strings to values the model can ingest or remove the columns."
      ],
      "metadata": {
        "id": "YDZS1PeJGIJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datacount.printSchema()"
      ],
      "metadata": {
        "id": "Ae-akTZGEXqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7nogRdaCD7u"
      },
      "source": [
        "flights_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's drop the CANCELLATION_REASON column<br>\n",
        "And any rows missing data"
      ],
      "metadata": {
        "id": "kqqZjPceGYfx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRGyVWc-HnlV"
      },
      "source": [
        "flights_df = flights_df.drop(flights_df.CANCELLATION_REASON)\n",
        "flights_df = flights_df.na.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select from the \"CANCELLED\" column only unique values. <br>\n",
        "We can keep only the rows where the flight was not cancelled<br>\n",
        "\n",
        "1= flight cancelled<br>\n",
        "\n",
        "0=flight not cancelled"
      ],
      "metadata": {
        "id": "Jkul-AS_UqBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the model to predict if a flight will be cancelled. "
      ],
      "metadata": {
        "id": "-CVIXJ_V7f-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#flights_df.count"
      ],
      "metadata": {
        "id": "IfgnMIJwoH4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq1sdujBH566"
      },
      "source": [
        "#flights_df.select('CANCELLED').distinct().rdd.map(lambda r: r[0]).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights_df.show()"
      ],
      "metadata": {
        "id": "2UnrjVWdlXPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our features for the model will be only those inputs that will halp make a prediction. "
      ],
      "metadata": {
        "id": "Vioo1vA9GxCX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeTswmtlID66"
      },
      "source": [
        "feature_cols = ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_TIME', 'ARRIVAL_DELAY', 'FLIGHT_NUMBER', 'DISTANCE', 'DIVERTED']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols"
      ],
      "metadata": {
        "id": "s7NwkF0XG3Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VectorAssembler = A feature transformer that merges multiple columns into a vector column.<br>\n",
        "Next we add the features column to the dataset"
      ],
      "metadata": {
        "id": "FAta7RQDYLe0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K52xVd6EIb4J"
      },
      "source": [
        "flights_df = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").transform(flights_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#datacount=flights_df.groupBy(\"CANCELLED\").count()\n",
        "datacount.show()"
      ],
      "metadata": {
        "id": "7ecjIsTEHEI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the features column and the \"CANCELLED\" column to train the model "
      ],
      "metadata": {
        "id": "5Ado_7mY8Dt-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT9K47_UIfF8"
      },
      "source": [
        "flights_df.select(\"Cancelled\", \"features\").show(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to build our indexers, split our dataset into 70 % for our training set and 30 % for our test set, define the parameters of our model and finally link everything together into a pipeline which we’’ll later use to actually run the model:"
      ],
      "metadata": {
        "id": "CpiNK46i8WC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "StringIndexer: converts a single column to an index column "
      ],
      "metadata": {
        "id": "_RXL2coiZiId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a labelIndexer\n",
        "labelIndexer = StringIndexer(inputCol=\"CANCELLED\", outputCol=\"indexedLabel\").fit(flights_df)"
      ],
      "metadata": {
        "id": "uUwCyN7FFHuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the indexed feature vector\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(flights_df)"
      ],
      "metadata": {
        "id": "vfNEyO-T4PeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and tests sets\n",
        "(trainingData, testData) = flights_df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n"
      ],
      "metadata": {
        "id": "hcKfzshL4S4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The (random forest) algorithm establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or mean of the output from various trees. Increasing the number of trees increases the precision of the outcom"
      ],
      "metadata": {
        "id": "qeUZwKP55HLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the RandomForest model\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)"
      ],
      "metadata": {
        "id": "3HMXcmN84W6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", \n",
        "     featuresCol=\"indexedFeatures\", \n",
        "     numTrees=RF_NUM_TREES,\n",
        "     maxDepth=3, \n",
        "     seed = 1,\n",
        "     featureSubsetStrategy=\"sqrt\",\n",
        "     impurity='gini') "
      ],
      "metadata": {
        "id": "E2l7DqlVbWh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwjvRtYxIqZJ"
      },
      "source": [
        "# Chain indexers and the forest models in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model = pipeline.fit(trainingData)"
      ],
      "metadata": {
        "id": "mtcbyNn84bf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma_tQZWDItgo"
      },
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(testData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihapr9xuIwKe"
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
        "print(f\"Accuracy = {accuracy:g}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What should we check with regards to the data? <br>\n",
        "Do we have balanced data"
      ],
      "metadata": {
        "id": "eCKi0bbt8nwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.select(\"prediction\",\"CANCELLED\",\"features\" ).show()"
      ],
      "metadata": {
        "id": "M2RNtT4H4h4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.show()"
      ],
      "metadata": {
        "id": "xZc5wdheXguf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Confusion Matrix**"
      ],
      "metadata": {
        "id": "uvoFw1hvRrS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = predictions.groupBy('indexedLabel','prediction').count().sort('indexedLabel','prediction')\n",
        "print(\"Confusion matrix\")\n",
        "cm.show()"
      ],
      "metadata": {
        "id": "K-FqH_CoaePe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TN = cm.filter((cm.indexedLabel == 0) & (cm.prediction == 0)).collect()[0][2] #True negative\n",
        "FP = cm.filter((cm.indexedLabel == 0) & (cm.prediction == 1)).collect()[0][2] #False positive\n",
        "FN = cm.filter((cm.indexedLabel == 1) & (cm.prediction == 0)).collect()[0][2] #Flase negative\n",
        "TP = cm.filter((cm.indexedLabel == 1) & (cm.prediction == 1)).collect()[0][2] #True positive\n",
        "\n",
        "N=TN+FP+TP+FN\n",
        "\n",
        "Prev = (TP + FN) / N  \n",
        "Sens = TPR = Recall = TP / (TP + FN) \n",
        "Esp  = TN / (TN + FP) #= (1 - FPR) \n",
        "Precision = PPV = TP / (TP + FP) \n",
        "Acc = (TP+TN) / N  \n",
        "\n",
        "print(\"Metrics:\")\n",
        "print('Prevalence=',round(Prev,2))\n",
        "print('Sensitivity=',round(Sens,2))\n",
        "print('Especificity=',round(Esp,2))\n",
        "print('Recall=',round(Recall,2))\n",
        "print('Precision=',round(Precision,2))\n",
        "print('Accuracy=',round(Acc,2))"
      ],
      "metadata": {
        "id": "m1mYsyhtRuU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC Curve**"
      ],
      "metadata": {
        "id": "uNob7DS4StW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "secondelement=udf(lambda v:float(v[1]),FloatType())\n",
        "pred2=pred.withColumn('probs',secondelement('probability'))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "import seaborn as sns\n",
        "\n",
        "pd5 = pred2.sample(False,0.1).select('label','probs').toPandas()\n",
        "\n",
        "fpr, tpr, _ = roc_curve( pd5['label'], pd5['probs'])\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlabel('1 - Esp (FPR)')\n",
        "plt.ylabel('Sens (TPR)')\n",
        "plt.title('Curva ROC')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Iqka4T1wSr8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Area Ubder the Curve (AUC)**"
      ],
      "metadata": {
        "id": "n7_zwa92SzkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator as BCE\n",
        "\n",
        "print('AUC=',BCE(metricName=\"areaUnderROC\",rawPredictionCol = 'probability').evaluate(prediction))"
      ],
      "metadata": {
        "id": "rJy2zyzoSyr_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}