{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySparkDelayedFlights",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySparkDelayedFlights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WcC7uUz7S4u"
      },
      "source": [
        "https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3\n",
        "\n",
        "\n",
        "https://medium.com/grabngoinfo/install-pyspark-3-on-google-colab-the-easy-way-577ec4a2bcd8\n",
        "\n",
        "Check for the latest version of Spark at https://pypi.org/project/pyspark/#history\n",
        "\n",
        "https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp\n",
        "\n",
        "\n",
        "https://grabngoinfo.com/install-pyspark-3-on-google-colab-the-easy-way/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unset SPARK_HOME"
      ],
      "metadata": {
        "id": "JohKVc6wP437"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cagBRT/PySpark.git"
      ],
      "metadata": {
        "id": "vn0ujiMgQXc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "gkVZEiasOkh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "vvON8-UoOqQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "7ZLp5GP1OtxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD5KzNGA7RZH"
      },
      "source": [
        "import os\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "print(\"If no error - everything is working\")\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv55s8h_8hev"
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_SqXaIST-eQ"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "#%cd cloned-repo\n",
        "#!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k67iOVTXEfEg"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11D3rDu9AcDV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1RFC0UzFEX5"
      },
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "# Set up constants\n",
        "FLIGHTS= \"/content/gdrive/My Drive/flights.csv\" \n",
        "AIRPORTS= \"/content/cloned-repo/airports.csv\"\n",
        "AIRLINES= \"/content/cloned-repo/airlines.csv\"\n",
        "APP_NAME = \"Flight Delays\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141109\n",
        "TRAINING_DATA_RATIO = 0.7\n",
        "RF_NUM_TREES = 8\n",
        "RF_MAX_DEPTH = 4\n",
        "RF_NUM_BINS = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0V8_n8LFJAb"
      },
      "source": [
        "# Connect to the Spark server\n",
        "spark = SparkSession.builder.appName(APP_NAME).master(FLIGHTS).getOrCreate()\n",
        "\n",
        "# Load datasets\n",
        "flights_df = spark.read.options(header=\"true\",inferschema = \"true\").csv(FLIGHTS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-raO-RCFzje"
      },
      "source": [
        "print(f\"The shape is {flights_df.count():d} rows by {len(flights_df.columns):d} columns.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj-kmimfGHoS"
      },
      "source": [
        "null_counts = flights_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)for c in flights_df.columns]).toPandas().to_dict(orient='records')\n",
        "print(f\"We have {sum(null_counts[0].values()):d} null values in this dataset.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7nogRdaCD7u"
      },
      "source": [
        "flights_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRGyVWc-HnlV"
      },
      "source": [
        "flights_df = flights_df.drop(flights_df.CANCELLATION_REASON)\n",
        "flights_df = flights_df.na.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gykHxAmtH2tS"
      },
      "source": [
        "flights_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq1sdujBH566"
      },
      "source": [
        "flights_df.select('CANCELLED').distinct().rdd.map(lambda r: r[0]).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeTswmtlID66"
      },
      "source": [
        "feature_cols = ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_TIME', 'ARRIVAL_DELAY', 'FLIGHT_NUMBER', 'DISTANCE', 'DIVERTED']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K52xVd6EIb4J"
      },
      "source": [
        "flights_df = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").transform(flights_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT9K47_UIfF8"
      },
      "source": [
        "flights_df.select(\"Cancelled\", \"features\").show(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwjvRtYxIqZJ"
      },
      "source": [
        "# Generate a labelIndexer\n",
        "labelIndexer = StringIndexer(inputCol=\"CANCELLED\", outputCol=\"indexedLabel\").fit(flights_df)\n",
        "\n",
        "# Generate the indexed feature vector\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(flights_df)\n",
        "    \n",
        "# Split the data into training and tests sets\n",
        "(trainingData, testData) = flights_df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
        "\n",
        "# Train the RandomForest model\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
        "\n",
        "# Chain indexers and the forest models in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma_tQZWDItgo"
      },
      "source": [
        "# Train model\n",
        "model = pipeline.fit(trainingData)\n",
        "# Make predictions\n",
        "predictions = model.transform(testData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihapr9xuIwKe"
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
        "print(f\"Accuracy = {accuracy:g}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}