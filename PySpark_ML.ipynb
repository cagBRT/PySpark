{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPOxCXH4wmijBnQRinSy53v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySpark_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/machine-learning-on-a-large-scale-2eef3bb749ee"
      ],
      "metadata": {
        "id": "FiPFeiYx5D4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning with PySpark"
      ],
      "metadata": {
        "id": "VXuEoGiHL6P5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook you use PySpark and create, train, test an ML model. <br>\n",
        "The dataset used for this example is not large enough to require PySpark, but is used to demonstrate the techniques. "
      ],
      "metadata": {
        "id": "bVyHZu_9Meve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "-eeTXez8SVvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "#%cd cloned-repo\n",
        "#!ls"
      ],
      "metadata": {
        "id": "i5FpmmE6PEmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"/content/cloned-repo/\"+str(num)+ \".png\" , width=640)"
      ],
      "metadata": {
        "id": "6i7Y9PHmPMEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used for this notebook comes from the PySpark library. <br><br>\n",
        "PySpark is designed to handle large datasets that are not feasible to work with on a single machine using pandas. If you have a dataset that is too large to fit in memory, or if you need to perform iterative or distributed computations, PySpark is the better choice.<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "qg0xwOoxM6bG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trVICwCSSC9e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml import Pipeline, Estimator, Transformer, Model, PipelineModel\n",
        "import pyspark.ml.feature as MFT\n",
        "import pyspark.ml.functions as MF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the Spark session"
      ],
      "metadata": {
        "id": "j6DONyyrNnf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a spark session, UI at http://localhost:4040/executors/\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "        .appName('learn')\n",
        "        .config('spark.driver.memory', '8g')\n",
        "        .master('local[4]',)\n",
        "        .config('spark.sql.execution.arrow.pyspark.enabled', True)\n",
        "        .config('spark.sql.execution.arrow.pyspark.fallback.enabled', False)\n",
        "        .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "zH6OEue3NioT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "b89uQUgNSots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Iris dataset form the Seaborn library"
      ],
      "metadata": {
        "id": "c2U-e4LkNt4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = sns.load_dataset('iris')"
      ],
      "metadata": {
        "id": "aB2gdmYDSODj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is small: 150 rows and four features"
      ],
      "metadata": {
        "id": "cec-h4JoN2oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "IT3D1vbhNzNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, hue='species')"
      ],
      "metadata": {
        "id": "kfsOido1Sueg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a binary class datset: virginica and not virginica<br>\n",
        "There are:<br>\n",
        ">100 not virginica data points<br>\n",
        "50 virginica data points\n"
      ],
      "metadata": {
        "id": "tJ-YyJAROAbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_binary = df.assign(species=df['species'].where(df['species']=='virginica', 'not virginica'))\n",
        "\n",
        "df_binary['species'].value_counts()"
      ],
      "metadata": {
        "id": "2p-Mz2CPSxlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_binary = spark.createDataFrame(df_binary)\n",
        "df_binary.printSchema()"
      ],
      "metadata": {
        "id": "wrHxWbYNS0qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_binary.head(15)"
      ],
      "metadata": {
        "id": "pJg8hs50OMI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a features column of the 4 features "
      ],
      "metadata": {
        "id": "lFfgTxBhO_Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pack the features into a vector (this is a transformer)\n",
        "feature_assembler = MFT.VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')\n",
        "\n",
        "tmp = feature_assembler.transform(df_binary)\n",
        "tmp.show(n=3, truncate=False)"
      ],
      "metadata": {
        "id": "jLxh5GTqS81W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The pprint module** in Python is a utility module that you can use to print data structures in a readable, pretty way. It's a part of the standard library that's especially useful for debugging code dealing with API requests, large JSON files, and data in general."
      ],
      "metadata": {
        "id": "8s03DgOmPMxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(tmp.schema['features'].metadata)\n",
        "# {'ml_attr': {'attrs': {'numeric': [{'idx': 0, 'name': 'sepal_length'},\n",
        "#                                    {'idx': 1, 'name': 'sepal_width'},\n",
        "#                                    {'idx': 2, 'name': 'petal_length'},\n",
        "#                                    {'idx': 3, 'name': 'petal_width'}]},\n",
        "#              'num_attrs': 4}}\n",
        "\n",
        "pprint([col['name'] for col in tmp.schema['features'].metadata['ml_attr']['attrs']['numeric']])\n",
        "# ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']"
      ],
      "metadata": {
        "id": "4kSmKpggTA1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalize the data using the MinMaxScaler function**<br>\n",
        "Rescale each feature individually to a common range [min, max] linearly using column summary statistics, which is also known as min-max normalization or Rescaling"
      ],
      "metadata": {
        "id": "M7ccobi5P1X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pack the features into a vector (this is a transformer)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#feature_assembler = MFT.VectorAssembler(inputCols=['sepal_length', 'sepal_width', \n",
        "#                                                   'petal_length', 'petal_width'], outputCol='features')\n",
        "\n",
        "# create a min max scaler (this is an estimator)\n",
        "minMax_scaler = MFT.MinMaxScaler(min=0., max=1., inputCol='features', outputCol='features_scaled')\n",
        "minmax_scaler_model=minMax_scaler.fit(tmp)\n",
        "\n",
        "tmp = minmax_scaler_model.transform(tmp)\n",
        "tmp.show(n=3, truncate=False)"
      ],
      "metadata": {
        "id": "vQFZgd26TF4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a pipeline to automate assembling the features and the data normalization."
      ],
      "metadata": {
        "id": "SufvMIq6PgnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple pipeline, which acts as an estimator. A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer. "
      ],
      "metadata": {
        "id": "yGopWIYPJjEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Pipeline.fit() is called, the stages are executed in order. If a stage is an Estimator, its Estimator.fit() method will be called on the input dataset to fit a model. Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage. If a stage is a Transformer, its Transformer.transform() method will be called to produce the dataset for the next stage. The fitted model from a Pipeline is a PipelineModel, "
      ],
      "metadata": {
        "id": "jebvQ5wSJu-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate the pipeline\n",
        "pipeline = Pipeline(stages=[feature_assembler, minMax_scaler])\n",
        "\n",
        "# fit the pipeline\n",
        "pipeline_model = pipeline.fit(df_binary)\n",
        "\n",
        "tmp = pipeline_model.transform(df_binary)\n",
        "tmp.show(n=3, truncate=False)\n"
      ],
      "metadata": {
        "id": "oYigzcsw3iB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StringIndexer encodes a string column of labels to a column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels).\n",
        "\n",
        "By default, this is ordered by label frequencies so the most frequent label gets index 0."
      ],
      "metadata": {
        "id": "W1zcOyjbWHjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# string indexer (this is a transformer)\n",
        "fromlabelsModel = MFT.StringIndexerModel.from_labels([\"not virginica\", \"virginica\"],\n",
        "                                                     inputCol=\"species\", outputCol=\"species_indexed\",\n",
        "                                                     handleInvalid=\"error\")\n"
      ],
      "metadata": {
        "id": "eKcTD_fa3mk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fromlabelsModel.labels)\n",
        "print(fromlabelsModel.extractParamMap())"
      ],
      "metadata": {
        "id": "9jbur2fIWiq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the stages of the Pipeline"
      ],
      "metadata": {
        "id": "6-DK9mPCJhIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add a stage to the pipeline\n",
        "pipeline = pipeline.setStages(pipeline.getStages() + [fromlabelsModel])\n",
        "\n",
        "pipeline.getStages()\n",
        "# [VectorAssembler_d5758e224502, MinMaxScaler_f73f5af81295, StringIndexerModel: uid=strIdx_f67ba1442b96, handleInvalid=error]   "
      ],
      "metadata": {
        "id": "-esXF-OsJSYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#See the stages of the pipline\n",
        "print(pipeline.getStages())"
      ],
      "metadata": {
        "id": "1naM1sm_J6WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data randomly into training and test sets"
      ],
      "metadata": {
        "id": "zY-lY3oQKKwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df_binary.randomSplit([0.5, 0.5], seed=11)\n",
        "train.cache()"
      ],
      "metadata": {
        "id": "zcGYXJoZ3tbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(5)"
      ],
      "metadata": {
        "id": "mqqZdvXrKbsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(5)"
      ],
      "metadata": {
        "id": "ncL1q-hfKjXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pyspark.sql.DataFrame.repartition() method is used to increase or decrease the RDD/DataFrame partitions by number of partitions or by single column name or multiple column names. This function takes 2 parameters; numPartitions and *cols, when one is specified the other is optional. repartition() is a wider transformation that involves shuffling of the data hence,\n",
        "it is considered an expensive operation\n",
        "\n"
      ],
      "metadata": {
        "id": "urw_xutEKopV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look in this directory and you can see the partitions created <br>\n",
        "/tmp/partition.csv"
      ],
      "metadata": {
        "id": "ouKKUqGrMGwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PySpark DataFrame\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
        "  ]\n",
        "\n",
        "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.show()\n",
        "\n",
        "# Write to CSV file\n",
        "df.write.mode(\"overwrite\").csv(\"/tmp/partition.csv\")"
      ],
      "metadata": {
        "id": "mJGmwonkLkCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run the code below you will see the number of partitions changes to 3"
      ],
      "metadata": {
        "id": "4qssht3kMmJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.repartition(numPartitions=3)\n",
        "print(df2.rdd.getNumPartitions())\n",
        "\n",
        "# Write DataFrame to CSV file\n",
        "df2.write.mode(\"overwrite\").csv(\"/tmp/partition.csv\")"
      ],
      "metadata": {
        "id": "ohjp6hTzMilh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for npar in [10, 20, 30]:\n",
        "    df_binary = df_binary.repartition(npar)\n",
        "    train, test = df_binary.randomSplit([0.5, 0.5], seed=14)\n",
        "    print(train.count(), test.count())"
      ],
      "metadata": {
        "id": "6toT6knY3wHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and train the model<br>\n",
        "Use the pipeline"
      ],
      "metadata": {
        "id": "RzvkDv_2NNMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first attempt to build a model (whole code given)\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "# binary classification\n",
        "df_binary = df.assign(species=df['species'].where(df['species']=='virginica', 'not virginica'))\n",
        "df_binary = spark.createDataFrame(df_binary)\n",
        "\n",
        "# pack the features into a vector (this is a transformer)\n",
        "# we do not use all features to allow some room for model tuning\n",
        "# feature_assembler = MFT.VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')\n",
        "feature_assembler = MFT.VectorAssembler(inputCols=['sepal_width', 'petal_width'], outputCol='features')\n",
        "\n",
        "# create a min max scaler (this is an estimator)\n",
        "minMax_scaler = MFT.MinMaxScaler(min=0., max=1., inputCol='features', outputCol='features_scaled')\n",
        "\n",
        "# string indexer (this is a transformer)\n",
        "labels = [\"not virginica\", \"virginica\"]\n",
        "fromlabelsModel = MFT.StringIndexerModel.from_labels(labels,\n",
        "                                                     inputCol=\"species\", outputCol=\"species_indexed\",\n",
        "                                                     handleInvalid=\"error\")\n",
        "\n",
        "# create a binomial logistic regression model (this is an estimator)\n",
        "lr = LogisticRegression(featuresCol='features_scaled', labelCol='species_indexed', predictionCol='prediction')\n",
        "\n",
        "# build the pipeline\n",
        "pipeline = Pipeline(stages=[feature_assembler, minMax_scaler, fromlabelsModel, lr])\n",
        "\n",
        "# cache the dataset to ensure deterministic behaviour and ensure fast model fitting\n",
        "df_binary.cache()\n",
        "train, test = df_binary.randomSplit([0.2, 0.8], seed=8)\n",
        "\n",
        "# fit the model\n",
        "pipeline_model = pipeline.fit(train)\n",
        "\n",
        "# apply the model to the test set\n",
        "results = pipeline_model.transform(test)\n",
        "results = MFT.IndexToString(inputCol='prediction', outputCol='species_predicted', labels=labels).transform(results)"
      ],
      "metadata": {
        "id": "tYJ6oPMe35dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the confusion matrix manually (for the training set)\n",
        "confusion_matrix = (results.groupby('species').pivot('species_predicted').count().toPandas()\n",
        "                    .set_index('species').sort_index(axis='index', ascending=False).sort_index(axis='columns', ascending=False)\n",
        "                    .fillna(0).astype(int)\n",
        "                    )"
      ],
      "metadata": {
        "id": "Jqaeb8TA3-Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the confusion matrix\n",
        "ax = sns.heatmap(confusion_matrix, cmap='Blues', annot=True, cbar=False)\n",
        "ax.set_xlabel('Prediction')\n",
        "ax.set_ylabel('True value')"
      ],
      "metadata": {
        "id": "Fs2tIgXz4Ak3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model (test set)\n",
        "lr_model = pipeline_model.stages[-1]\n",
        "metrics = lr_model.evaluate(results.select('features_scaled', 'species_indexed'))\n",
        "\n",
        "print(f'Recall, sensitivity or true positive rate {metrics.recallByLabel[1]:.4f}')\n",
        "print(f'Precision or positive predictive value {metrics.precisionByLabel[1]:.4f}')\n",
        "print(f'Specificity or true negative rate {1-metrics.falsePositiveRateByLabel[1]:.4f}')\n",
        "print(f'False positive rate or (1 - specificity) {metrics.falsePositiveRateByLabel[1]:.4f}')\n"
      ],
      "metadata": {
        "id": "vd8nrcql4Fwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the ROC curve manually\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "results = results.withColumn('probability', MF.vector_to_array(F.col('probability'))).withColumn('prob 0', F.col('probability')[0]).withColumn('prob 1', F.col('probability')[1])\n",
        "raw = results.select(MF.vector_to_array(F.col('rawPrediction')).alias('raw prediction')).select(F.col('raw prediction')[0].alias('raw prediction 0'), F.col('raw prediction')[1].alias('raw prediction 1'))\n",
        "roc_manually = pd.DataFrame({'threshold':[0., 1.], 'FPR': [1., 0.], 'TPR': [1., 0.]})\n",
        "for threshold in tqdm(np.concatenate((np.arange(0.001, 0.02, 0.001),\n",
        "                                      np.arange(0.02, 1.0, 0.02),\n",
        "                                      np.arange(0.981, 1.00, 0.001)\n",
        "                                      ))):\n",
        "    TP = results.where((F.col('prob 1')>=threshold) & (F.col('species_indexed')==1.)).count()\n",
        "    TN = results.where((F.col('prob 1')<threshold) & (F.col('species_indexed')==0.)).count()\n",
        "    FP = results.where((F.col('prob 1')>=threshold) & (F.col('species_indexed')==0.)).count()\n",
        "    FN = results.where((F.col('prob 1')<threshold) & (F.col('species_indexed')==1.)).count()\n",
        "    TPR = TP/(TP+FN)\n",
        "    FPR = FP/(TN+FP)\n",
        "    print(TP, TN, FP, FN)\n",
        "    roc_manually = pd.concat([roc_manually, pd.DataFrame([[threshold, FPR, TPR]], columns=roc_manually.columns)], axis='index', ignore_index=True)\n",
        "\n",
        "# compute the ROC curve using the PySpark API\n",
        "roc_pyspark = metrics.roc.toPandas()\n",
        "\n",
        "# visualise the ROC curve\n",
        "ax = sns.scatterplot(data=roc_manually.sort_values(by='FPR'), x='FPR', y='TPR', marker='o', ec='b', facecolor='none', label='ROC (manually)')\n",
        "sns.scatterplot(data=roc_pyspark, x='FPR', y='TPR', color='r', marker='+', ax=ax, label='ROC (PySpark)')\n",
        "ax.set_xlabel('False positive rate (1 - specificity)')\n",
        "ax.set_xlabel('True positive rate (sensitivity)')"
      ],
      "metadata": {
        "id": "RVKa28eZ4KsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"roc-curve-v2\")"
      ],
      "metadata": {
        "id": "sV3hDWXEPWVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model performance\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='species_indexed', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "areaUnderROC = evaluator.evaluate(results.select('species_indexed', 'rawPrediction'))\n",
        "\n",
        "print(f'The area under the ROC curve for the test set is {areaUnderROC:.4f}')"
      ],
      "metadata": {
        "id": "obo_6uh84gSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pyspark cache()** method is used to cache the intermediate results of the transformation so that other transformation runs on top of cached will perform faster. Caching the result of the transformation is one of the optimization tricks to improve the performance of the long-running PySpark applications/jobs.\n",
        "\n"
      ],
      "metadata": {
        "id": "UsB0MQKAJdoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caching a DataFrame that can be reused for multi-operations will significantly improve any PySpark job. Below are the benefits of cache().<br>\n",
        "\n",
        "Cost-efficient – Spark computations are very expensive hence reusing the computations are used to save cost.<br>\n",
        "Time-efficient – Reusing repeated computations saves lots of time.\n",
        "Execution time – Saves execution time of the job and we can perform more jobs on the same cluster.\n"
      ],
      "metadata": {
        "id": "q4G63CdYJlNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# complete code, incuding cross-validation\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "# binary classification\n",
        "df_binary = df.assign(species=df['species'].where(df['species']=='virginica', 'not virginica'))\n",
        "df_binary = spark.createDataFrame(df_binary)\n",
        "\n",
        "# cache the dataset to ensure deterministic behaviour and ensure fast model fitting\n",
        "df_binary.cache()\n",
        "train, test = df_binary.randomSplit([0.6, 0.4], seed=8)\n",
        "\n",
        "# pack the features into a vector (this is a transformer)\n",
        "feature_assembler = MFT.VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')\n",
        "\n",
        "# create a min max scaler (this is an estimator)\n",
        "minMax_scaler = MFT.MinMaxScaler(min=0., max=1., inputCol='features', outputCol='features_scaled')\n",
        "\n",
        "# string indexer (this is a transformer)\n",
        "labels = [\"not virginica\", \"virginica\"]\n",
        "fromlabelsModel = MFT.StringIndexerModel.from_labels(labels,\n",
        "                                                     inputCol=\"species\", outputCol=\"species_indexed\",\n",
        "                                                     handleInvalid=\"error\")\n",
        "\n",
        "# create a binomial logistic regression model (this is an estimator)\n",
        "lr = LogisticRegression(featuresCol='features_scaled', labelCol='species_indexed', predictionCol='prediction')\n",
        "\n",
        "# build the pipeline\n",
        "pipeline = Pipeline(stages=[feature_assembler, minMax_scaler, fromlabelsModel, lr])\n",
        "\n",
        "# build the parameter map (grid)\n",
        "from itertools import combinations, chain\n",
        "from pprint import pprint\n",
        "from time import time\n",
        "cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "cols_combinations = list(chain(combinations(cols, 2), combinations(cols, 3), combinations(cols, 4)))\n",
        "pprint(cols_combinations)\n",
        "# [('sepal_length', 'sepal_width'),\n",
        "#  ('sepal_length', 'petal_length'),\n",
        "#  ('sepal_length', 'petal_width'),\n",
        "#  ('sepal_width', 'petal_length'),\n",
        "#  ('sepal_width', 'petal_width'),\n",
        "#  ('petal_length', 'petal_width'),\n",
        "#  ('sepal_length', 'sepal_width', 'petal_length'),\n",
        "#  ('sepal_length', 'sepal_width', 'petal_width'),\n",
        "#  ('sepal_length', 'petal_length', 'petal_width'),\n",
        "#  ('sepal_width', 'petal_length', 'petal_width'),\n",
        "#  ('sepal_length', 'sepal_width', 'petal_length', 'petal_width')]\n",
        "param_grid = (ParamGridBuilder()\n",
        "              .addGrid(lr.elasticNetParam, [0., 0.5, 1.])\n",
        "              .addGrid(lr.regParam, [0., 0.1, 1., 2., 5.])\n",
        "              .addGrid(feature_assembler.inputCols, cols_combinations)\n",
        "              .build())\n",
        "\n",
        "# evaluate the model performance\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='species_indexed', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "\n",
        "# setup the cross validation estimator\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        "                    estimatorParamMaps=param_grid,\n",
        "                    evaluator=evaluator,\n",
        "                    numFolds=4,\n",
        "                    seed=8,\n",
        "                    collectSubModels=True)\n",
        "\n",
        "# fit the model using cross-validation and grid search\n",
        "start_time = time()\n",
        "cv_model = cv.fit(train)\n",
        "\n",
        "print(f'Hyper-parameter tuning using 4-fold validation took {time()-start_time: 0.2f} sec')\n",
        "# Hyper-parameter tuning using 4-fold validation took  348.70 sec"
      ],
      "metadata": {
        "id": "U98tz9k04pb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the best model and the optimised hyper-parameters\n",
        "best_model = cv_model.bestModel\n",
        "print(f'features maintained in the best model {best_model.stages[0].getInputCols()}')\n",
        "print(f'elastic net parameter in the best model {best_model.stages[-1].getElasticNetParam()}')\n",
        "print(f'regularization parameter in the best model {best_model.stages[-1].getRegParam()}')\n",
        "# features maintained in the best model ['sepal_width', 'petal_width']\n",
        "# elastic net parameter in the best model 0.0\n",
        "# regularization parameter in the best model 0.0\n",
        "\n",
        "# apply the model to the training set\n",
        "results = best_model.transform(train)\n",
        "results = MFT.IndexToString(inputCol='prediction', outputCol='species_predicted', labels=labels).transform(results)\n",
        "\n",
        "# compute the confusion matrix manually (for the training set)\n",
        "confusion_matrix = (results.groupby('species').pivot('species_predicted').count().toPandas()\n",
        "                    .set_index('species').sort_index(axis='index', ascending=False).sort_index(axis='columns', ascending=False)\n",
        "                    .fillna(0).astype(int)\n",
        "                    )\n",
        "print(confusion_matrix)\n"
      ],
      "metadata": {
        "id": "s1Ka5f234v9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the coefficients\n",
        "results = best_model.transform(train)\n",
        "results = MFT.IndexToString(inputCol='prediction', outputCol='species_predicted', labels=labels).transform(results)\n",
        "coef_names = ['intercept'] + [x['name'] for x in results.schema['features'].metadata['ml_attr']['attrs']['numeric']]\n",
        "coef_values = [best_model.stages[-1].intercept] + list(best_model.stages[-1].coefficients)\n",
        "coefs = pd.Series(coef_values, index=coef_names)\n",
        "\n",
        "print(coefs)"
      ],
      "metadata": {
        "id": "TxwocbE845Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw the decision boundary and show the training set as a scatter plot\n",
        "results = results.select('features_scaled', 'species', 'species_predicted')\n",
        "results = results.withColumn('sepal_width_scaled', MF.vector_to_array(F.col('features_scaled'))[0])\n",
        "results = results.withColumn('petal_width_scaled', MF.vector_to_array(F.col('features_scaled'))[1])\n",
        "results_pd = results.select('sepal_width_scaled', 'petal_width_scaled', 'species', 'species_predicted').toPandas()\n",
        "\n",
        "ax = sns.scatterplot(data=results_pd, x='sepal_width_scaled', y='petal_width_scaled', hue='species')\n",
        "xs = np.linspace(results_pd['sepal_width_scaled'].min(), results_pd['sepal_width_scaled'].max(), 3)\n",
        "ys = (-coefs['intercept'] - coefs['sepal_width']*xs)/coefs['petal_width']\n",
        "sns.lineplot(x=xs, y=ys, ax=ax)\n",
        "ax.lines[0].set_linestyle(\"--\")\n",
        "ax.lines[0].set_color(\"k\")\n",
        "ax.set_xlabel('sepal width (scaled)')\n",
        "ax.set_ylabel('petal width (scaled)')"
      ],
      "metadata": {
        "id": "i6vnlkzY4-iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multinomial logistic regression (softmax)\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset('iris')\n",
        "\n",
        "# binary classification\n",
        "df_multinomial = spark.createDataFrame(df)\n",
        "\n",
        "# pack the features into a vector (this is a transformer)\n",
        "feature_assembler = MFT.VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')\n",
        "\n",
        "# create a min max scaler (this is an estimator)\n",
        "minMax_scaler = MFT.MinMaxScaler(min=0., max=1., inputCol='features', outputCol='features_scaled')\n",
        "\n",
        "# string indexer (this is a transformer)\n",
        "labels = ['setosa', 'versicolor', 'virginica']\n",
        "fromlabelsModel = MFT.StringIndexerModel.from_labels(labels,\n",
        "                                                     inputCol=\"species\", outputCol=\"species_indexed\",\n",
        "                                                     handleInvalid=\"error\")\n",
        "\n",
        "# create a binomial logistic regression model (this is an estimator)\n",
        "lr = LogisticRegression(featuresCol='features_scaled', labelCol='species_indexed', predictionCol='prediction')\n",
        "\n",
        "# build the pipeline\n",
        "pipeline = Pipeline(stages=[feature_assembler, minMax_scaler, fromlabelsModel, lr])\n",
        "\n",
        "# cache the dataset to ensure deterministic behaviour and ensure fast model fitting\n",
        "df_multinomial.cache()\n",
        "train, test = df_multinomial.randomSplit([0.8, 0.2], seed=8)\n",
        "\n",
        "# fit the model\n",
        "pipeline_model = pipeline.fit(train)\n",
        "\n",
        "# apply the model to the test set\n",
        "results = pipeline_model.transform(test)\n",
        "results = MFT.IndexToString(inputCol='prediction', outputCol='species_predicted', labels=labels).transform(results)\n",
        "\n",
        "# compute the confusion matrix manually (for the test set)\n",
        "confusion_matrix = (results.groupby('species').pivot('species_predicted').count().toPandas()\n",
        "                    .set_index('species').sort_index(axis='index', ascending=False).sort_index(axis='columns', ascending=False)\n",
        "                    .fillna(0).astype(int)\n",
        "                    )\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "id": "snKvhyEX5e85"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}