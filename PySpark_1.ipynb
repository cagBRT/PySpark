{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPpyZWRDI8/46Mtf900twED",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySpark_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "#%cd cloned-repo\n",
        "#!ls"
      ],
      "metadata": {
        "id": "CgduyP92Uv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"/content/cloned-repo/\"+str(num)+ \".png\" , width=640)"
      ],
      "metadata": {
        "id": "fS83pNJuVBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"PySpark\")"
      ],
      "metadata": {
        "id": "_50-2cB1BAw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7wQrL7jJVKM"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "#getOrCreate gets or creates a session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "BYmRHz5mJeWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "4fmKVmnaJipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(\"If no error - everything is working\")\n"
      ],
      "metadata": {
        "id": "o3WlhnO8JmLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a PySpark session**"
      ],
      "metadata": {
        "id": "6Pl8Ci6SBila"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "y3LaVsmtIcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "metadata": {
        "id": "d8swmeMGJsUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zPEx-WZvJy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FTExGxCy-KcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RDD stands for Resilient Distributed Dataset**,<br>\n",
        "these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.<br>\n",
        "**Once you create an RDD you cannot change it.** <br>\n",
        "RDDs are fault tolerant as well, hence in case of any failure, they recover automatically. You can apply multiple operations on these RDDs to achieve a certain task.<br>\n",
        "\n",
        "To apply operations on these RDD's, there are two ways −\n",
        "\n",
        ">Transformation<br>\n",
        "Action<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "1b07lSdRGl6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation** − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations."
      ],
      "metadata": {
        "id": "fXYhBDrTHJRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action** − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver."
      ],
      "metadata": {
        "id": "jTfI7xBfHPhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use RDD**<br>\n",
        "Use an RDDs in situations where:<br>\n",
        "\n",
        "Data is unstructured.<br><br> Unstructured data sources such as media or text streams benefit from the performance advantages RDDs offer.<br><br>\n",
        "Transformations are low-level.<br><br> Data manipulation should be fast and straightforward when nearer to the data source.<br><br>\n",
        "Schema is unimportant. Since RDDs do not impose schemas, use them when accessing specific data by column or attribute is not relevant."
      ],
      "metadata": {
        "id": "HOX7P_EQP_tI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a PySpark RDD**"
      ],
      "metadata": {
        "id": "-TL2w1hrHpON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Sd4rZ-CgGkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1**<br>\n",
        "Create an RDD called favorites. <br>\n",
        "It should contain 10 of your favorite things. "
      ],
      "metadata": {
        "id": "FUcZKVKKInxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment\n"
      ],
      "metadata": {
        "id": "qOO-QSrGI3cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**count()**"
      ],
      "metadata": {
        "id": "9Lw1cbuGIAUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = words.count()\n",
        "print (\"Number of elements in RDD -> %i\" % (counts))"
      ],
      "metadata": {
        "id": "6iElYitOH_uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2**<br>\n",
        "Count the number of elements in the RDD you created called favorites"
      ],
      "metadata": {
        "id": "JnXrC65wJEbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 2"
      ],
      "metadata": {
        "id": "6OesifzaJR_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collect()**<br>\n",
        "All the elements in the RDD are returned"
      ],
      "metadata": {
        "id": "omopEW4eJWAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coll = words.collect()\n",
        "print (\"Elements in RDD -> %s\" % (coll))"
      ],
      "metadata": {
        "id": "j4KInDmpJYI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3**<br>\n",
        "Collect the elements in favorites"
      ],
      "metadata": {
        "id": "GjMNmzCAJj7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 3"
      ],
      "metadata": {
        "id": "fbQfejLMJpO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(f)**\n",
        "A filter returns the elements that meet the requested condition"
      ],
      "metadata": {
        "id": "dfHq9T7UBkGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the words dataset is searched for those elements containing 'spark'"
      ],
      "metadata": {
        "id": "sBOCWlEcBm5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words RDD:<br>\n",
        ">[\"scala\", <br>\n",
        "   \"java\",<br> \n",
        "   \"hadoop\", <br>\n",
        "   \"spark\", <br>\n",
        "   \"akka\",<br>\n",
        "   \"spark vs hadoop\", <br>\n",
        "   \"pyspark\",<br>\n",
        "   \"pyspark and spark\"]<br>"
      ],
      "metadata": {
        "id": "pEs3GTsdBpAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_filter = words.filter(lambda x: 'spark' in x)\n",
        "filtered = words_filter.collect()\n",
        "print (\"Fitered RDD -> %s\" % (filtered))"
      ],
      "metadata": {
        "id": "KUSK1-ndBtq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(f)**\n",
        "A filter returns the elements that meet the requested condition"
      ],
      "metadata": {
        "id": "Jqg1Z5-GDAWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the words RDD is searched for those elements containing 'spark'"
      ],
      "metadata": {
        "id": "6T3eB9qqDELo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words dataset:<br>\n",
        ">[\"scala\", <br>\n",
        "   \"java\",<br> \n",
        "   \"hadoop\", <br>\n",
        "   \"spark\", <br>\n",
        "   \"akka\",<br>\n",
        "   \"spark vs hadoop\", <br>\n",
        "   \"pyspark\",<br>\n",
        "   \"pyspark and spark\"]<br>"
      ],
      "metadata": {
        "id": "1m6T9Qd7C7m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_filter = words.filter(lambda x: 'spark' in x)\n",
        "filtered = words_filter.collect()\n",
        "print (\"Fitered RDD -> %s\" % (filtered))"
      ],
      "metadata": {
        "id": "daHqpO02DNgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 6**:<br>\n",
        "Find all the elements in words that contain the letter 'k'"
      ],
      "metadata": {
        "id": "tixnhP5ADV_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 6"
      ],
      "metadata": {
        "id": "h4yWBxFdDWZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataFrames**\n",
        "RDD offers low-level control over data, Dataset and DataFrame APIs bring structure and high-level abstractions. Keep in mind that transformations from an RDD to a Dataset or DataFrame are easy to execute"
      ],
      "metadata": {
        "id": "hwEUdRvKPNso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use Datasets**<br>\n",
        "Use Datasets in situations where:<br>\n",
        "\n",
        "Data requires a structure. <br><br>\n",
        "DataFrames infer a schema on structured and semi-structured data.<br><br>\n",
        "Transformations are high-level. If your data requires high-level processing, columnar functions, and SQL queries, use Datasets and DataFrames.<br><br>\n",
        "A high degree of type safety is necessary. Compile-time type-safety takes full advantage of the speed of development and efficiency."
      ],
      "metadata": {
        "id": "FDxg11BpQXjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "df.select(\"Name\").show()"
      ],
      "metadata": {
        "id": "xhH6wWimOiOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "DXL4hfwLJ9K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create and use a function on the dataset**"
      ],
      "metadata": {
        "id": "KVlVmp7sCOrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(df,col):\n",
        "  df.select(col).show()"
      ],
      "metadata": {
        "id": "EYPRnfv-PLCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(df,\"Name\")"
      ],
      "metadata": {
        "id": "tOv4Y3eOR-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(df,\"Seqno\")"
      ],
      "metadata": {
        "id": "MUeE8zZVR7Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 4**<br>\n",
        "Use the given dataset<br>\n",
        "Show the dataset<br>\n",
        "Show each col individually<br>\n",
        "Count the number of rows<br>"
      ],
      "metadata": {
        "id": "Hm0VM938p3__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 4\n",
        "columns = [\"Race_event\",\"Winner\"]\n",
        "data_assignment4 = [\n",
        "    (\"100yd\", \"baby smith\"),\n",
        "    (\"quarter Mile\", \"bonny jones\"),\n",
        "    (\"half mile\", \"cutie topper\"),\n",
        "    (\"mile\", \"legs mccarty\"),\n",
        "    (\"two mile\", \"speed hari\"),\n",
        "    (\"five mile\", \"betsy boop\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "p3T7SLQmqKko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**foreach(f)**\n",
        "Returns only those elements which meet the condition of the function inside foreach. In the following example, we call a print function in foreach, which prints all the elements in the RDD.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZB2vpB4JrOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this instance we use the accumulator function to gather values from the dataset<br><br>\n",
        "An accumulator is created from an initial value v by calling SparkContext.accumulator(v). <br>\n",
        "\n",
        "Tasks running on the cluster can then add to it using the add method or the += operator (in Scala and Python). However, they cannot read its value."
      ],
      "metadata": {
        "id": "Ut22aCZqCb2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accumulators are for ints and floats. To do other values, use the AccumulatorParam."
      ],
      "metadata": {
        "id": "eowx5TggDG2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def addup(df):\n",
        "  accum=spark.sparkContext.accumulator(0)\n",
        "  df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "  print (accum.value)"
      ],
      "metadata": {
        "id": "uNf6z16-TThO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addup(df)"
      ],
      "metadata": {
        "id": "MvU56W48TcWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below we comment out the accumulator(0) function. <br>\n",
        "Note what happens"
      ],
      "metadata": {
        "id": "-M_XMNx_DbLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def addup(df):\n",
        "  #accum=spark.sparkContext.accumulator(0)\n",
        "  df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "  print(accum.value)"
      ],
      "metadata": {
        "id": "F-Li-wlZT9m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addup(df)"
      ],
      "metadata": {
        "id": "fvZkPMeZUBD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using foreach**"
      ],
      "metadata": {
        "id": "Sxn8Cgn_EFPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# foreach() with accumulator Example\n",
        "accum=spark.sparkContext.accumulator(0)\n",
        "df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "print(accum.value) #Accessed by driver"
      ],
      "metadata": {
        "id": "5XDibKvlRPGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 5**<br>\n",
        "Use the given dataset<br>\n",
        "Add the appropriate columns"
      ],
      "metadata": {
        "id": "Q0bFibDls5Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"candy\", \"sales San Jose\",\"sales for California\",\"sales for US\" ]\n",
        "data_assignment5 = [\n",
        "    (\"snickers\",\"34\",\"124\",\"564\"),\n",
        "    (\"goodbar\",\"23\",\"445\",\"876\"),\n",
        "    (\"twix\",\"12\",\"765\",\"1234\"),\n",
        "    (\"m&m\",\"365\",\"987\",\"121234\"),\n",
        "    (\"mars\",\"56\",\"87\",\"234657\"),\n",
        "    (\"red hots\",\"7\",\"67\",\"989877\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "kjsTH_lNtODg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(f)**\n",
        "A filter returns the elements that meet the requested condition"
      ],
      "metadata": {
        "id": "mCOSxId4-OM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the words dataset is searched for those elements containing 'spark'"
      ],
      "metadata": {
        "id": "Qsj4-FKpEwpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max()**"
      ],
      "metadata": {
        "id": "XOFh6OCQGYS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max"
      ],
      "metadata": {
        "id": "q4HCx8qIGa4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Number\",\"letter\"]\n",
        "dataNum=[(\"44\",\"a\"),(\"5\",\"z\"),(\"7.3\",\"c\"),(\"5.5\",\"x\")]\n",
        "dNum=spark.createDataFrame(data=dataNum,schema=columns)"
      ],
      "metadata": {
        "id": "WtqPmiUoGdZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dNum.show()"
      ],
      "metadata": {
        "id": "nP6XPgrLI6Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxValue = dNum.agg(max(\"Number\")).collect()[0][0]\n",
        "print(\"maxValue: \",maxValue)"
      ],
      "metadata": {
        "id": "oVWI9Gf5GzNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 7**<br>\n",
        "Use the given dataset, <br>\n",
        "Find the maxvalue for all appropriate colmuns"
      ],
      "metadata": {
        "id": "c3XW6Exu3nvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 7\n",
        "columns = [\"candy\", \"sales San Jose\",\"sales for California\",\"sales for US\" ]\n",
        "data_assignment7 = [\n",
        "    (\"snickers\",\"34\",\"124\",\"564\"),\n",
        "    (\"goodbar\",\"23\",\"445\",\"876\"),\n",
        "    (\"twix\",\"12\",\"765\",\"1234\"),\n",
        "    (\"m&m\",\"365\",\"987\",\"121234\"),\n",
        "    (\"mars\",\"56\",\"87\",\"234657\"),\n",
        "    (\"red hots\",\"7\",\"67\",\"989877\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "7KqOUm8D3-le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Partitioning?"
      ],
      "metadata": {
        "id": "hX3JNlT8Q-HK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**map()** transforms the RDD by applying the lambda function to every element in the dataframe. "
      ],
      "metadata": {
        "id": "546rLf3bFjjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "9IX4bl2kFxht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this instance we are mapping  a second value (23) to each element"
      ],
      "metadata": {
        "id": "CzPXbH8J5Zqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_map = rdd.map(lambda x: (x, 23))\n",
        "mapping = words_map.collect()\n",
        "print (\"Key value pair -> %s\" % (mapping))"
      ],
      "metadata": {
        "id": "ya2g9upjF5GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for element in words_map.collect():\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "Az9ZEpVG4nyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"partitions=\",rdd.getNumPartitions())\n",
        "print(\"count=\",words_map.count())\n",
        "print(words_map.collect())"
      ],
      "metadata": {
        "id": "yBoaS9beN_iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 8**<br>\n",
        "Add another element to words_map"
      ],
      "metadata": {
        "id": "FMXBMvIl6BOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 8"
      ],
      "metadata": {
        "id": "Itr-4wXv6J5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "words_map2 = words_map.map(lambda x: (x, \"windy\"))\n",
        "mapping = words_map2.collect()\n",
        "print (\"Key value pair -> %s\" % (mapping))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GfGKzQA16ifJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduce()**<br>\n",
        "After performing the specified commutative and associative binary operation, the element in the RDD is returned<br>\n",
        "\n",
        "For example: <br>\n",
        "reduce(lambda x, y : x + y, [1,2,3,4,5]) ==> (((1+2)+3)+4)+5)\n",
        "\n"
      ],
      "metadata": {
        "id": "esuEQh7HRgAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "\n",
        "listRdd = spark.sparkContext.parallelize([-1,33,3,4,5,3,2])\n",
        "print(\"output min using binary : \", listRdd.reduce(min))\n",
        "print(\"output add using binary : \", listRdd.reduce(add))\n",
        "\n",
        "print(\"output max using binary : \", listRdd.max())\n"
      ],
      "metadata": {
        "id": "A8P5ItSeRhIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  listRdd = spark.sparkContext.parallelize((1,2,3,4,5,3,2))\n",
        "  print(\"output min using binary : \",listRdd.reduce( min ))\n",
        "  print(\"output max using binary : \",listRdd.reduce(max))\n",
        "  print(\"output sum using binary : \",listRdd.reduce(add))"
      ],
      "metadata": {
        "id": "zCUunoka8zjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 9**"
      ],
      "metadata": {
        "id": "3XtqhzRvR2P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"candy\", \"sales San Jose\",\"sales for California\",\"sales for US\" ]\n",
        "data_assignment9 = [\n",
        "    (\"snickers\",\"34\",\"124\",\"564\"),\n",
        "    (\"goodbar\",\"23\",\"445\",\"876\"),\n",
        "    (\"twix\",\"12\",\"765\",\"1234\"),\n",
        "    (\"m&m\",\"365\",\"987\",\"121234\"),\n",
        "    (\"mars\",\"56\",\"87\",\"234657\"),\n",
        "    (\"red hots\",\"7\",\"67\",\"989877\")\n",
        "    ]\n",
        "\n",
        "adding = data_assignment9.reduce(add)\n",
        "print (\"Adding all the elements -> %i\" % (adding))"
      ],
      "metadata": {
        "id": "-eERBvp063tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**join**<br>"
      ],
      "metadata": {
        "id": "If0XH5PeR2Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
        "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
        "joined = x.join(y)\n",
        "final = joined.collect()\n",
        "print (\"Join RDD -> %s\" % (final))"
      ],
      "metadata": {
        "id": "N5oyFxpyR5SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cache()**"
      ],
      "metadata": {
        "id": "NLnlR_XNSOUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ") \n",
        "words.cache() \n",
        "caching = words.persist().is_cached \n",
        "print( \"Words got chached > %s\" % (caching))"
      ],
      "metadata": {
        "id": "3NDzfFEnSOil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**broadcast()**<br>\n",
        "A Broadcast variable has an attribute called value, which stores the data and is used to return a broadcasted value.\n",
        "\n"
      ],
      "metadata": {
        "id": "1nhXgdnJSdbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
        "data = words_new.value \n",
        "print (\"Stored data -> %s\" % (data) )\n",
        "elem = words_new.value[2] \n",
        "print (\"Printing a particular element in RDD -> %s\" % (elem))"
      ],
      "metadata": {
        "id": "BLKu_9xnSdr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accumulator()**<br>"
      ],
      "metadata": {
        "id": "bGtyqe5vS7Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = sc.accumulator(10) \n",
        "def f(x): \n",
        "   global num \n",
        "   num+=x \n",
        "rdd = sc.parallelize([20,30,40,50]) \n",
        "rdd.foreach(f) \n",
        "final = num.value \n",
        "print (\"Accumulated value is -> %i\" % (final))"
      ],
      "metadata": {
        "id": "__BSp36pS80R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GroupBy()"
      ],
      "metadata": {
        "id": "RmsHMDgxJHaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a simple dataset"
      ],
      "metadata": {
        "id": "bkmyjrFKJOq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
        "  ]\n"
      ],
      "metadata": {
        "id": "0MrACiPYJLLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the data schema - the column names"
      ],
      "metadata": {
        "id": "smDfN4kZJU22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]"
      ],
      "metadata": {
        "id": "6TKcabbYJTk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataframe"
      ],
      "metadata": {
        "id": "5t0Gs1Q-Jamn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "ZjOLFMMzJayW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment:** <br>\n",
        "Create a dataframe with the following column headings. <br>\n",
        "Create at least 10 rows <br>\n",
        "\n",
        "Name, state, team_name, event_name, race_time1, race_time2, race_time3"
      ],
      "metadata": {
        "id": "xQPLCCGCSq7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using groupby()<br>\n",
        "Group the dataframe into three categories: Sales, Finance, Marketing<br>\n",
        "Notice the salary is the sum of the values in each category"
      ],
      "metadata": {
        "id": "oZDKuWEGKDGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
      ],
      "metadata": {
        "id": "ny3_V0whKDRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groupy by number of employees"
      ],
      "metadata": {
        "id": "7VZF8oRHKhjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").count().show()"
      ],
      "metadata": {
        "id": "8BqkMsZ0KhuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").min(\"salary\").show()"
      ],
      "metadata": {
        "id": "DMdMjIKQLS4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").max(\"salary\").show()"
      ],
      "metadata": {
        "id": "opp3m1xYLS6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").mean( \"salary\").show()"
      ],
      "metadata": {
        "id": "OUu1NQRTLa3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groupby on multiple columns<br>\n",
        "Group by department and state"
      ],
      "metadata": {
        "id": "pbXsjBGDLiVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GroupBy on multiple columns\n",
        "df.groupBy(\"department\",\"state\") \\\n",
        "    .sum(\"salary\",\"bonus\") \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "id": "J0b_khOhLk6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Doing multiple aggrates at the same time**"
      ],
      "metadata": {
        "id": "EXrzBZxvL-93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,avg,max\n",
        "df.groupBy(\"department\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
        "         max(\"bonus\").alias(\"max_bonus\") \\\n",
        "     ) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "id": "e3GAS7pFL_Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring Spark"
      ],
      "metadata": {
        "id": "_f5gSlkbUrEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Following are some of the most commonly used attributes of SparkConf −\n",
        "\n",
        ">set(key, value) − To set a configuration property.\n",
        "\n",
        ">setMaster(value) − To set the master URL.\n",
        "\n",
        ">setAppName(value) − To set an application name.\n",
        "\n",
        ">get(key, defaultValue=None) − To get a configuration value of a key.\n",
        "\n",
        ">setSparkHome(value) − To set Spark installation path on worker nodes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dhl7lGGVUyPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf"
      ],
      "metadata": {
        "id": "XuP1CWw1VBpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
        "#sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "IEEUPpLLUvVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get(filename)<br>\n",
        "getrootdirectory()"
      ],
      "metadata": {
        "id": "h4Cq_bnWV15d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "finddistance = \"/content/cloned-repo/airlines.csv\"\n",
        "finddistancename = \"finddistance.R\"\n",
        "sc.addFile(finddistance)\n",
        "print (\"Absolute Path -> %s\" % SparkFiles.get(finddistancename))"
      ],
      "metadata": {
        "id": "J2HtPVm-VcRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Storage Levels**"
      ],
      "metadata": {
        "id": "yP6drY9_V_Os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISK_ONLY = StorageLevel(True, False, False, False, 1)\n",
        "\n",
        "DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n",
        "\n",
        "MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n",
        "\n",
        "MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n",
        "\n",
        "MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\n",
        "\n",
        "MEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\n",
        "\n",
        "MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n",
        "\n",
        "MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n",
        "\n",
        "MEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\n",
        "\n",
        "MEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\n",
        "\n",
        "OFF_HEAP = StorageLevel(True, True, True, False, 1)"
      ],
      "metadata": {
        "id": "72LlRhNlWByx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "rdd1 = sc.parallelize([1,2])\n",
        "rdd1.persist( pyspark.StorageLevel.MEMORY_AND_DISK_2 )\n",
        "rdd1.getStorageLevel()\n",
        "print(rdd1.getStorageLevel())\n"
      ],
      "metadata": {
        "id": "7dWB1h8kWBSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}