{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO7bVSvYLssjSDTT8xOFhrb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySpark_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "#%cd cloned-repo\n",
        "#!ls"
      ],
      "metadata": {
        "id": "CgduyP92Uv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"/content/cloned-repo/\"+str(num)+ \".png\" , width=640)"
      ],
      "metadata": {
        "id": "fS83pNJuVBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7wQrL7jJVKM"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "#getOrCreate gets or creates a session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "BYmRHz5mJeWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "4fmKVmnaJipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(\"If no error - everything is working\")\n"
      ],
      "metadata": {
        "id": "o3WlhnO8JmLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "y3LaVsmtIcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "metadata": {
        "id": "d8swmeMGJsUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zPEx-WZvJy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FTExGxCy-KcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD stands for Resilient Distributed Dataset**,<br>\n",
        "these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.<br>\n",
        "**Once you create an RDD you cannot change it.** <br>\n",
        "RDDs are fault tolerant as well, hence in case of any failure, they recover automatically. You can apply multiple operations on these RDDs to achieve a certain task.<br>\n",
        "\n",
        "To apply operations on these RDD's, there are two ways −\n",
        "\n",
        ">Transformation<br>\n",
        "Action<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "1b07lSdRGl6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation** − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations."
      ],
      "metadata": {
        "id": "fXYhBDrTHJRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action** − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver."
      ],
      "metadata": {
        "id": "jTfI7xBfHPhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a PySPark RDD**"
      ],
      "metadata": {
        "id": "-TL2w1hrHpON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Sd4rZ-CgGkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1**<br>\n",
        "Create an RDD called favorites. <br>\n",
        "It should contain 10 of your favorite things. "
      ],
      "metadata": {
        "id": "FUcZKVKKInxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment\n"
      ],
      "metadata": {
        "id": "qOO-QSrGI3cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**count()**"
      ],
      "metadata": {
        "id": "9Lw1cbuGIAUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = words.count()\n",
        "print (\"Number of elements in RDD -> %i\" % (counts))"
      ],
      "metadata": {
        "id": "6iElYitOH_uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2**<br>\n",
        "Count the number of elements in the RDD you created called favorites"
      ],
      "metadata": {
        "id": "JnXrC65wJEbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 2"
      ],
      "metadata": {
        "id": "6OesifzaJR_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collect()**"
      ],
      "metadata": {
        "id": "omopEW4eJWAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coll = words.collect()\n",
        "print (\"Elements in RDD -> %s\" % (coll))"
      ],
      "metadata": {
        "id": "j4KInDmpJYI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3**<br>\n",
        "Collect the elements in favorites"
      ],
      "metadata": {
        "id": "GjMNmzCAJj7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 3"
      ],
      "metadata": {
        "id": "fbQfejLMJpO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**foreach(f)**\n",
        "Returns only those elements which meet the condition of the function inside foreach. In the following example, we call a print function in foreach, which prints all the elements in the RDD.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZB2vpB4JrOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "df.select(\"Name\").show()"
      ],
      "metadata": {
        "id": "xhH6wWimOiOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "DXL4hfwLJ9K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(df,col):\n",
        "  df.select(col).show()"
      ],
      "metadata": {
        "id": "EYPRnfv-PLCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(df,\"Name\")"
      ],
      "metadata": {
        "id": "tOv4Y3eOR-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(df,\"Seqno\")"
      ],
      "metadata": {
        "id": "MUeE8zZVR7Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def addup(df):\n",
        "  accum=spark.sparkContext.accumulator(0)\n",
        "  df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "  print (accum.value)"
      ],
      "metadata": {
        "id": "uNf6z16-TThO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addup(df)"
      ],
      "metadata": {
        "id": "MvU56W48TcWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def addup(df):\n",
        "  #accum=spark.sparkContext.accumulator(0)\n",
        "  df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "  print(accum.value)"
      ],
      "metadata": {
        "id": "F-Li-wlZT9m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addup(df)"
      ],
      "metadata": {
        "id": "fvZkPMeZUBD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# foreach() with accumulator Example\n",
        "accum=spark.sparkContext.accumulator(0)\n",
        "df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "print(accum.value) #Accessed by driver"
      ],
      "metadata": {
        "id": "5XDibKvlRPGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Pandas dataframe**"
      ],
      "metadata": {
        "id": "mCOSxId4-OM8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAXif1auJ7jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StringIndexer<br>\n",
        "Use stringIndexer when you want a column identified as catagorical data. "
      ],
      "metadata": {
        "id": "WwdwZja686ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StringIndexer maps a string column to a index column that will be treated as a categorical column by spark. The indices start with 0 and are ordered by label frequencies. If it is a numerical column, the column will be cast as a string column and then indexed by StringIndexer.\n",
        "\n",
        "There are three steps to implement the StringIndexer\n",
        "\n",
        "Build the StringIndexer model: specify the input column and output column names.\n",
        "Learn the StringIndexer model: fit the model with your data.\n",
        "Execute the indexing: call the transform function to execute the indexing process."
      ],
      "metadata": {
        "id": "lWuvYjaeLjtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# build indexer\n",
        "string_indexer = StringIndexer(inputCol='x1', outputCol='indexed_x1')\n",
        "\n",
        "# learn the model\n",
        "string_indexer_model = string_indexer.fit(df)\n",
        "\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model.transform(df)\n",
        "\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "metadata": {
        "id": "cilw1neYKGP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the indexed_x1 column the value 'b' is the most numerous so it get the value 0<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "HjBX3uz49ggr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment<br>\n",
        "Column x2 contains catagorical data. <br>\n",
        "Convert it to data the ML model can use. "
      ],
      "metadata": {
        "id": "_es_es6992BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment\n"
      ],
      "metadata": {
        "id": "hkDWR0R8-CYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "# build indexer\n",
        "string_indexer = StringIndexer(inputCol='x2', outputCol='indexed_x2')\n",
        "\n",
        "# learn the model\n",
        "string_indexer_model = string_indexer.fit(df_stringindexer)\n",
        "\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model.transform(df_stringindexer)\n",
        "\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ti1sWuVv9Nmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}