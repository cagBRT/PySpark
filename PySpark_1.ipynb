{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXfvHyVIGxoO1gTA2PU2uy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySpark_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "%cd cloned-repo\n",
        "#!ls"
      ],
      "metadata": {
        "id": "CgduyP92Uv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"/content/cloned-repo/\"+str(num)+ \".png\" , width=640)"
      ],
      "metadata": {
        "id": "fS83pNJuVBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing."
      ],
      "metadata": {
        "id": "o0rlV0J5mg-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"PySpark\")"
      ],
      "metadata": {
        "id": "_50-2cB1BAw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7wQrL7jJVKM"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "#getOrCreate gets or creates a session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "BYmRHz5mJeWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "4fmKVmnaJipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(\"If no error - everything is working\")\n"
      ],
      "metadata": {
        "id": "o3WlhnO8JmLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a PySpark session**"
      ],
      "metadata": {
        "id": "6Pl8Ci6SBila"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "y3LaVsmtIcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "metadata": {
        "id": "d8swmeMGJsUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zPEx-WZvJy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FTExGxCy-KcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"distributed computing\")"
      ],
      "metadata": {
        "id": "rEnVn-3UjY8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"parallel computing\")"
      ],
      "metadata": {
        "id": "v3Gdugy5jRk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A graphical representation of Amdahl's law. The speedup of a program from parallelization is limited by how much of the program can be parallelized. For example, if 90% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 10 times no matter how many processors are used."
      ],
      "metadata": {
        "id": "X2s-Cg2bogYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"amdahls law chart\")"
      ],
      "metadata": {
        "id": "mRjjYI8SmE66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume that a task has two independent parts, A and B. Part B takes roughly 25% of the time of the whole computation. By working very hard, one may be able to make this part 5 times faster, but this only reduces the time for the whole computation by a little. In contrast, one may need to perform less work to make part A be twice as fast. This will make the computation much faster than by optimizing part B, even though part B's speedup is greater by ratio, (5 times versus 2 times)."
      ],
      "metadata": {
        "id": "_s_qbsdfozXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"speedup\")"
      ],
      "metadata": {
        "id": "-sr4oeyLpPz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RDD stands for Resilient Distributed Dataset**,<br>\n",
        "these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.<br>\n",
        "**Once you create an RDD you cannot change it.** <br>\n",
        "RDDs are fault tolerant as well, hence in case of any failure, they recover automatically. You can apply multiple operations on these RDDs to achieve a certain task.<br>\n",
        "\n",
        "To apply operations on these RDD's, there are two ways −\n",
        "\n",
        ">Transformation<br>\n",
        "Action<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "1b07lSdRGl6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation** − These are the operations, which are applied on a RDD **to create a new RDD**. Filter, groupBy and map are the examples of transformations."
      ],
      "metadata": {
        "id": "fXYhBDrTHJRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action** − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver."
      ],
      "metadata": {
        "id": "jTfI7xBfHPhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use RDD**<br>\n",
        "Use an RDDs in situations where:<br>\n",
        "\n",
        "Data is unstructured.<br><br> Unstructured data sources such as media or text streams benefit from the performance advantages RDDs offer.<br><br>\n",
        "Transformations are low-level.<br><br> Data manipulation should be fast and straightforward when nearer to the data source.<br><br>\n",
        "Schema is unimportant. Since RDDs do not impose schemas, use them when accessing specific data by column or attribute is not relevant."
      ],
      "metadata": {
        "id": "HOX7P_EQP_tI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a PySpark RDD**"
      ],
      "metadata": {
        "id": "-TL2w1hrHpON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Sd4rZ-CgGkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1**<br>\n",
        "Create an RDD called favorites. <br>\n",
        "It should contain 10 of your favorite things. "
      ],
      "metadata": {
        "id": "FUcZKVKKInxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment\n"
      ],
      "metadata": {
        "id": "qOO-QSrGI3cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**count()**"
      ],
      "metadata": {
        "id": "9Lw1cbuGIAUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = words.count()\n",
        "print (\"Number of elements in RDD -> %i\" % (counts))"
      ],
      "metadata": {
        "id": "6iElYitOH_uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2**<br>\n",
        "Count the number of elements in the RDD you created called favorites"
      ],
      "metadata": {
        "id": "JnXrC65wJEbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 2"
      ],
      "metadata": {
        "id": "6OesifzaJR_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collect()**<br>\n",
        "All the elements in the RDD are returned"
      ],
      "metadata": {
        "id": "omopEW4eJWAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coll = words.collect()\n",
        "print (\"Elements in RDD -> %s\" % (coll))"
      ],
      "metadata": {
        "id": "j4KInDmpJYI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3**<br>\n",
        "Collect the elements in favorites"
      ],
      "metadata": {
        "id": "GjMNmzCAJj7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 3"
      ],
      "metadata": {
        "id": "093gD6mEYqWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Lambda Example Begin\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Y73tbtvVYpIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lambda functions**<br>\n",
        "A lambda function is a small anonymous function.<br>\n",
        "\n",
        "A lambda function can take any number of arguments, but can only have one expression.\n"
      ],
      "metadata": {
        "id": "aUj9lPFtWwyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax for lambda functions:<br>\n",
        ">lambda arguments : expression"
      ],
      "metadata": {
        "id": "M06lqGDZW9kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a is the input to lambda, \n",
        "#returnOfFunction is the input+5\n",
        "returnOfFunction=lambda a: a+5\n",
        "#print the return of the function when a=3\n",
        "print(returnOfFunction(3))"
      ],
      "metadata": {
        "id": "_RqGwURWW6u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a function with more than one input\n",
        "returnOfFunction=lambda a,b: a+b+100\n",
        "print(returnOfFunction(3,4))"
      ],
      "metadata": {
        "id": "4QabfqTWYPq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lambda Assignment**<br>\n",
        "Write a lambda function that adds three input values<br>\n"
      ],
      "metadata": {
        "id": "_hqY6RWoXaLL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ylt5vKaaXeEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "returnOfFunction=lambda a,b,c: a+b+c\n",
        "print(returnOfFunction(3,4,5))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q8P1mZE8X0ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The power of lambda is better shown when you use them as an anonymous function inside another function."
      ],
      "metadata": {
        "id": "i9py9V75Yu01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. define a function using a lambda function"
      ],
      "metadata": {
        "id": "HvbMu8fDficu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def afunction(n):\n",
        "  return lambda a:a*n"
      ],
      "metadata": {
        "id": "hPUVrCPcfHuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "assign a variable to the output of the function<br>\n",
        "(the \"n\" value)"
      ],
      "metadata": {
        "id": "qzNvRrwzfmPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multiplier=afunction(4)"
      ],
      "metadata": {
        "id": "t8W1PbN9gLbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now assign a value to \"a\""
      ],
      "metadata": {
        "id": "Pr0NfNtZgTmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(multiplier(3))"
      ],
      "metadata": {
        "id": "29MeApA8fR1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment:**\n",
        "Create a function that uses a lambda function"
      ],
      "metadata": {
        "id": "eizuv67xg0pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment"
      ],
      "metadata": {
        "id": "MnjaS208g7PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Lambda Example End\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-wKRMzq-fFEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(f)**\n",
        "A filter returns the elements that meet the requested condition"
      ],
      "metadata": {
        "id": "dfHq9T7UBkGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the words dataset is searched for those elements containing 'spark'"
      ],
      "metadata": {
        "id": "sBOCWlEcBm5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words RDD:<br>\n",
        ">[\"scala\", <br>\n",
        "   \"java\",<br> \n",
        "   \"hadoop\", <br>\n",
        "   \"spark\", <br>\n",
        "   \"akka\",<br>\n",
        "   \"spark vs hadoop\", <br>\n",
        "   \"pyspark\",<br>\n",
        "   \"pyspark and spark\"]<br>"
      ],
      "metadata": {
        "id": "pEs3GTsdBpAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_filter = words.filter(lambda x: 'spark' in x)\n",
        "filtered = words_filter.collect()\n",
        "print (\"Fitered RDD -> %s\" % (filtered))"
      ],
      "metadata": {
        "id": "KUSK1-ndBtq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(f)**\n",
        "A filter returns the elements that meet the requested condition"
      ],
      "metadata": {
        "id": "Jqg1Z5-GDAWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the words RDD is searched for those elements containing 'spark'"
      ],
      "metadata": {
        "id": "6T3eB9qqDELo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words dataset:<br>\n",
        ">[\"scala\", <br>\n",
        "   \"java\",<br> \n",
        "   \"hadoop\", <br>\n",
        "   \"spark\", <br>\n",
        "   \"akka\",<br>\n",
        "   \"spark vs hadoop\", <br>\n",
        "   \"pyspark\",<br>\n",
        "   \"pyspark and spark\"]<br>"
      ],
      "metadata": {
        "id": "1m6T9Qd7C7m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_filter = words.filter(lambda x: 'spark' in x)\n",
        "filtered = words_filter.collect()\n",
        "print (\"Filtered RDD -> %s\" % (filtered))"
      ],
      "metadata": {
        "id": "daHqpO02DNgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 4**:<br>\n",
        "Find all the elements in words that contain the letter 'k'"
      ],
      "metadata": {
        "id": "tixnhP5ADV_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 4\n"
      ],
      "metadata": {
        "id": "h4yWBxFdDWZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "words_filter = words.filter(lambda x: 'k' in x)\n",
        "filtered = words_filter.collect()\n",
        "print (\"Filtered RDD -> %s\" % (filtered))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YRVHdfzchPgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduce()**<br>\n",
        "After performing the **specified commutative and associative binary operation**, the element in the RDD is returned<br>\n",
        "\n",
        "For example: <br>\n",
        "reduce(lambda x, y : x + y, [1,2,3,4,5]) ==> (((1+2)+3)+4)+5)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRznsUXFD5NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "\n",
        "listRdd = spark.sparkContext.parallelize([-1,33,3,4,5,3,2])\n",
        "print(\"output min using binary : \", listRdd.reduce(min))\n",
        "print(\"output add using binary : \", listRdd.reduce(add))\n",
        "\n",
        "print(\"output max using binary : \", listRdd.max())\n"
      ],
      "metadata": {
        "id": "rmax_VLUD-xT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}