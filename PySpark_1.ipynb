{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOaJddEkt16lH2oCnc9ZVGD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySpark_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "#%cd cloned-repo\n",
        "#!ls"
      ],
      "metadata": {
        "id": "CgduyP92Uv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"/content/cloned-repo/\"+str(num)+ \".png\" , width=640)"
      ],
      "metadata": {
        "id": "fS83pNJuVBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"PySpark\")"
      ],
      "metadata": {
        "id": "_50-2cB1BAw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7wQrL7jJVKM"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "#getOrCreate gets or creates a session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "BYmRHz5mJeWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "4fmKVmnaJipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(\"If no error - everything is working\")\n"
      ],
      "metadata": {
        "id": "o3WlhnO8JmLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a PySpark session"
      ],
      "metadata": {
        "id": "6Pl8Ci6SBila"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "y3LaVsmtIcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "metadata": {
        "id": "d8swmeMGJsUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zPEx-WZvJy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FTExGxCy-KcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD stands for Resilient Distributed Dataset**,<br>\n",
        "these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.<br>\n",
        "**Once you create an RDD you cannot change it.** <br>\n",
        "RDDs are fault tolerant as well, hence in case of any failure, they recover automatically. You can apply multiple operations on these RDDs to achieve a certain task.<br>\n",
        "\n",
        "To apply operations on these RDD's, there are two ways −\n",
        "\n",
        ">Transformation<br>\n",
        "Action<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "1b07lSdRGl6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation** − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations."
      ],
      "metadata": {
        "id": "fXYhBDrTHJRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action** − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver."
      ],
      "metadata": {
        "id": "jTfI7xBfHPhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use RDD**<br>\n",
        "Use an RDDs in situations where:<br>\n",
        "\n",
        "Data is unstructured.<br><br> Unstructured data sources such as media or text streams benefit from the performance advantages RDDs offer.<br><br>\n",
        "Transformations are low-level.<br><br> Data manipulation should be fast and straightforward when nearer to the data source.<br><br>\n",
        "Schema is unimportant. Since RDDs do not impose schemas, use them when accessing specific data by column or attribute is not relevant."
      ],
      "metadata": {
        "id": "HOX7P_EQP_tI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a PySpark RDD**"
      ],
      "metadata": {
        "id": "-TL2w1hrHpON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Sd4rZ-CgGkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1**<br>\n",
        "Create an RDD called favorites. <br>\n",
        "It should contain 10 of your favorite things. "
      ],
      "metadata": {
        "id": "FUcZKVKKInxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment\n"
      ],
      "metadata": {
        "id": "qOO-QSrGI3cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**count()**"
      ],
      "metadata": {
        "id": "9Lw1cbuGIAUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = words.count()\n",
        "print (\"Number of elements in RDD -> %i\" % (counts))"
      ],
      "metadata": {
        "id": "6iElYitOH_uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2**<br>\n",
        "Count the number of elements in the RDD you created called favorites"
      ],
      "metadata": {
        "id": "JnXrC65wJEbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 2"
      ],
      "metadata": {
        "id": "6OesifzaJR_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collect()**<br>\n",
        "All the elements in the RDD are returned"
      ],
      "metadata": {
        "id": "omopEW4eJWAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coll = words.collect()\n",
        "print (\"Elements in RDD -> %s\" % (coll))"
      ],
      "metadata": {
        "id": "j4KInDmpJYI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3**<br>\n",
        "Collect the elements in favorites"
      ],
      "metadata": {
        "id": "GjMNmzCAJj7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 3"
      ],
      "metadata": {
        "id": "fbQfejLMJpO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataFrames**\n",
        "RDD offers low-level control over data, Dataset and DataFrame APIs bring structure and high-level abstractions. Keep in mind that transformations from an RDD to a Dataset or DataFrame are easy to execute"
      ],
      "metadata": {
        "id": "hwEUdRvKPNso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use Datasets**<br>\n",
        "Use Datasets in situations where:<br>\n",
        "\n",
        "Data requires a structure. <br><br>\n",
        "DataFrames infer a schema on structured and semi-structured data.<br><br>\n",
        "Transformations are high-level. If your data requires high-level processing, columnar functions, and SQL queries, use Datasets and DataFrames.<br><br>\n",
        "A high degree of type safety is necessary. Compile-time type-safety takes full advantage of the speed of development and efficiency."
      ],
      "metadata": {
        "id": "FDxg11BpQXjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "df.select(\"Name\").show()"
      ],
      "metadata": {
        "id": "xhH6wWimOiOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "DXL4hfwLJ9K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create and use a function on the dataset**"
      ],
      "metadata": {
        "id": "KVlVmp7sCOrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(df,col):\n",
        "  df.select(col).show()"
      ],
      "metadata": {
        "id": "EYPRnfv-PLCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(df,\"Name\")"
      ],
      "metadata": {
        "id": "tOv4Y3eOR-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(df,\"Seqno\")"
      ],
      "metadata": {
        "id": "MUeE8zZVR7Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**foreach(f)**\n",
        "Returns only those elements which meet the condition of the function inside foreach. In the following example, we call a print function in foreach, which prints all the elements in the RDD.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZB2vpB4JrOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this instance we use the accumulator function to gather values from the dataset<br><br>\n",
        "An accumulator is created from an initial value v by calling SparkContext.accumulator(v). <br>\n",
        "\n",
        "Tasks running on the cluster can then add to it using the add method or the += operator (in Scala and Python). However, they cannot read its value."
      ],
      "metadata": {
        "id": "Ut22aCZqCb2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accumulators are for ints and floats. Do do other values, use the the AccumulatorParam."
      ],
      "metadata": {
        "id": "eowx5TggDG2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def addup(df):\n",
        "  accum=spark.sparkContext.accumulator(0)\n",
        "  df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "  print (accum.value)"
      ],
      "metadata": {
        "id": "uNf6z16-TThO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addup(df)"
      ],
      "metadata": {
        "id": "MvU56W48TcWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below we comment out the accumulator(0) function. <br>\n",
        "Note what happens"
      ],
      "metadata": {
        "id": "-M_XMNx_DbLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def addup(df):\n",
        "  #accum=spark.sparkContext.accumulator(0)\n",
        "  df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "  print(accum.value)"
      ],
      "metadata": {
        "id": "F-Li-wlZT9m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addup(df)"
      ],
      "metadata": {
        "id": "fvZkPMeZUBD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using foreach**"
      ],
      "metadata": {
        "id": "Sxn8Cgn_EFPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# foreach() with accumulator Example\n",
        "accum=spark.sparkContext.accumulator(0)\n",
        "df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "print(accum.value) #Accessed by driver"
      ],
      "metadata": {
        "id": "5XDibKvlRPGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(f)**\n",
        "A filter returns the elements that meet the requested condition"
      ],
      "metadata": {
        "id": "mCOSxId4-OM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the words dataset is searched for those elements containing 'spark'"
      ],
      "metadata": {
        "id": "Qsj4-FKpEwpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words dataset:<br>\n",
        ">[\"scala\", <br>\n",
        "   \"java\",<br> \n",
        "   \"hadoop\", <br>\n",
        "   \"spark\", <br>\n",
        "   \"akka\",<br>\n",
        "   \"spark vs hadoop\", <br>\n",
        "   \"pyspark\",<br>\n",
        "   \"pyspark and spark\"]<br>"
      ],
      "metadata": {
        "id": "Ok5D4QLHE5xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_filter = words.filter(lambda x: 'spark' in x)\n",
        "filtered = words_filter.collect()\n",
        "print (\"Fitered RDD -> %s\" % (filtered))"
      ],
      "metadata": {
        "id": "yAXif1auJ7jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment :<br>\n",
        "Find all the elements in favorites that contain the letter 'a'"
      ],
      "metadata": {
        "id": "G43acp5oFEen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment"
      ],
      "metadata": {
        "id": "-JfPTUi1FMDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max()**"
      ],
      "metadata": {
        "id": "XOFh6OCQGYS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max"
      ],
      "metadata": {
        "id": "q4HCx8qIGa4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Number\",\"letter\"]\n",
        "dataNum=[(\"44\",\"a\"),(\"5\",\"z\"),(\"7.3\",\"c\"),(\"5.5\",\"x\")]\n",
        "dNum=spark.createDataFrame(data=dataNum,schema=columns)"
      ],
      "metadata": {
        "id": "WtqPmiUoGdZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dNum.show()"
      ],
      "metadata": {
        "id": "nP6XPgrLI6Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxValue = dNum.agg(max(\"Number\")).collect()[0][0]\n",
        "print(\"maxValue: \",maxValue)"
      ],
      "metadata": {
        "id": "oVWI9Gf5GzNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Partitioning?"
      ],
      "metadata": {
        "id": "hX3JNlT8Q-HK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**map()** transforms the RDD by applying the lambda function to every element in the dataframe. "
      ],
      "metadata": {
        "id": "546rLf3bFjjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "9IX4bl2kFxht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_map = rdd.map(lambda x: (x, 1))\n",
        "mapping = rdd.collect()\n",
        "print (\"Key value pair -> %s\" % (mapping))"
      ],
      "metadata": {
        "id": "ya2g9upjF5GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_collected=rdd.collect()\n",
        "print(\"partitions=\",rdd.getNumPartitions())\n",
        "print(\"count=\",rdd.count())\n",
        "print(rdd_collected)"
      ],
      "metadata": {
        "id": "yBoaS9beN_iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "words_map = words.map(lambda x: (x, 1))\n",
        "mapping = words_map.collect()\n",
        "print \"Key value pair -> %s\" % (mapping)"
      ],
      "metadata": {
        "id": "wE0taNg4QvH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduce()**<br>\n",
        "After performing the specified commutative and associative binary operation, the element in the RDD is returned"
      ],
      "metadata": {
        "id": "esuEQh7HRgAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
        "adding = nums.reduce(add)\n",
        "print (\"Adding all the elements -> %i\" % (adding))\n"
      ],
      "metadata": {
        "id": "A8P5ItSeRhIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3XtqhzRvR2P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**join**<br>"
      ],
      "metadata": {
        "id": "If0XH5PeR2Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
        "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
        "joined = x.join(y)\n",
        "final = joined.collect()\n",
        "print (\"Join RDD -> %s\" % (final))"
      ],
      "metadata": {
        "id": "N5oyFxpyR5SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cache()**"
      ],
      "metadata": {
        "id": "NLnlR_XNSOUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ") \n",
        "words.cache() \n",
        "caching = words.persist().is_cached \n",
        "print( \"Words got chached > %s\" % (caching))"
      ],
      "metadata": {
        "id": "3NDzfFEnSOil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**broadcast()**<br>\n",
        "A Broadcast variable has an attribute called value, which stores the data and is used to return a broadcasted value.\n",
        "\n"
      ],
      "metadata": {
        "id": "1nhXgdnJSdbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
        "data = words_new.value \n",
        "print (\"Stored data -> %s\" % (data) )\n",
        "elem = words_new.value[2] \n",
        "print (\"Printing a particular element in RDD -> %s\" % (elem))"
      ],
      "metadata": {
        "id": "BLKu_9xnSdr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accumulator()**<br>"
      ],
      "metadata": {
        "id": "bGtyqe5vS7Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = sc.accumulator(10) \n",
        "def f(x): \n",
        "   global num \n",
        "   num+=x \n",
        "rdd = sc.parallelize([20,30,40,50]) \n",
        "rdd.foreach(f) \n",
        "final = num.value \n",
        "print (\"Accumulated value is -> %i\" % (final))"
      ],
      "metadata": {
        "id": "__BSp36pS80R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring Spark"
      ],
      "metadata": {
        "id": "_f5gSlkbUrEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Following are some of the most commonly used attributes of SparkConf −\n",
        "\n",
        ">set(key, value) − To set a configuration property.\n",
        "\n",
        ">setMaster(value) − To set the master URL.\n",
        "\n",
        ">setAppName(value) − To set an application name.\n",
        "\n",
        ">get(key, defaultValue=None) − To get a configuration value of a key.\n",
        "\n",
        ">setSparkHome(value) − To set Spark installation path on worker nodes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dhl7lGGVUyPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf"
      ],
      "metadata": {
        "id": "XuP1CWw1VBpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
        "#sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "IEEUPpLLUvVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get(filename)<br>\n",
        "getrootdirectory()"
      ],
      "metadata": {
        "id": "h4Cq_bnWV15d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "finddistance = \"/content/cloned-repo/airlines.csv\"\n",
        "finddistancename = \"finddistance.R\"\n",
        "sc.addFile(finddistance)\n",
        "print (\"Absolute Path -> %s\" % SparkFiles.get(finddistancename))"
      ],
      "metadata": {
        "id": "J2HtPVm-VcRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Storage Levels**"
      ],
      "metadata": {
        "id": "yP6drY9_V_Os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISK_ONLY = StorageLevel(True, False, False, False, 1)\n",
        "\n",
        "DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n",
        "\n",
        "MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n",
        "\n",
        "MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n",
        "\n",
        "MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\n",
        "\n",
        "MEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\n",
        "\n",
        "MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n",
        "\n",
        "MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n",
        "\n",
        "MEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\n",
        "\n",
        "MEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\n",
        "\n",
        "OFF_HEAP = StorageLevel(True, True, True, False, 1)"
      ],
      "metadata": {
        "id": "72LlRhNlWByx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "rdd1 = sc.parallelize([1,2])\n",
        "rdd1.persist( pyspark.StorageLevel.MEMORY_AND_DISK_2 )\n",
        "rdd1.getStorageLevel()\n",
        "print(rdd1.getStorageLevel())\n"
      ],
      "metadata": {
        "id": "7dWB1h8kWBSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfmL265NQioR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "# build indexer\n",
        "string_indexer = StringIndexer(inputCol='x2', outputCol='indexed_x2')\n",
        "\n",
        "# learn the model\n",
        "string_indexer_model = string_indexer.fit(df_stringindexer)\n",
        "\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model.transform(df_stringindexer)\n",
        "\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "metadata": {
        "id": "Ti1sWuVv9Nmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}