{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOJhUomOTrbByISOko03Q70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySpark_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "%cd cloned-repo\n",
        "#!ls"
      ],
      "metadata": {
        "id": "CgduyP92Uv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"/content/cloned-repo/\"+str(num)+ \".png\" , width=640)"
      ],
      "metadata": {
        "id": "fS83pNJuVBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing."
      ],
      "metadata": {
        "id": "o0rlV0J5mg-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"PySpark\")"
      ],
      "metadata": {
        "id": "_50-2cB1BAw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7wQrL7jJVKM"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "#getOrCreate gets or creates a session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "BYmRHz5mJeWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "4fmKVmnaJipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(\"If no error - everything is working\")\n"
      ],
      "metadata": {
        "id": "o3WlhnO8JmLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a PySpark session**"
      ],
      "metadata": {
        "id": "6Pl8Ci6SBila"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "y3LaVsmtIcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "metadata": {
        "id": "d8swmeMGJsUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zPEx-WZvJy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FTExGxCy-KcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"distributed computing\")"
      ],
      "metadata": {
        "id": "rEnVn-3UjY8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"parallel computing\")"
      ],
      "metadata": {
        "id": "v3Gdugy5jRk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A graphical representation of Amdahl's law. The speedup of a program from parallelization is limited by how much of the program can be parallelized. For example, if 90% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 10 times no matter how many processors are used."
      ],
      "metadata": {
        "id": "X2s-Cg2bogYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"amdahls law chart\")"
      ],
      "metadata": {
        "id": "mRjjYI8SmE66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume that a task has two independent parts, A and B. Part B takes roughly 25% of the time of the whole computation. By working very hard, one may be able to make this part 5 times faster, but this only reduces the time for the whole computation by a little. In contrast, one may need to perform less work to make part A be twice as fast. This will make the computation much faster than by optimizing part B, even though part B's speedup is greater by ratio, (5 times versus 2 times)."
      ],
      "metadata": {
        "id": "_s_qbsdfozXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"speedup\")"
      ],
      "metadata": {
        "id": "-sr4oeyLpPz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RDD stands for Resilient Distributed Dataset**,<br>\n",
        "these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.<br>\n",
        "**Once you create an RDD you cannot change it.** <br>\n",
        "RDDs are fault tolerant as well, hence in case of any failure, they recover automatically. You can apply multiple operations on these RDDs to achieve a certain task.<br>\n",
        "\n",
        "To apply operations on these RDD's, there are two ways −\n",
        "\n",
        ">Transformation<br>\n",
        "Action<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "1b07lSdRGl6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation** − These are the operations, which are applied on a RDD **to create a new RDD**. Filter, groupBy and map are the examples of transformations."
      ],
      "metadata": {
        "id": "fXYhBDrTHJRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action** − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver."
      ],
      "metadata": {
        "id": "jTfI7xBfHPhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use RDD**<br>\n",
        "Use an RDDs in situations where:<br>\n",
        "\n",
        "Data is unstructured.<br><br> Unstructured data sources such as media or text streams benefit from the performance advantages RDDs offer.<br><br>\n",
        "Transformations are low-level.<br><br> Data manipulation should be fast and straightforward when nearer to the data source.<br><br>\n",
        "Schema is unimportant. Since RDDs do not impose schemas, use them when accessing specific data by column or attribute is not relevant."
      ],
      "metadata": {
        "id": "HOX7P_EQP_tI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a PySpark RDD**"
      ],
      "metadata": {
        "id": "-TL2w1hrHpON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Sd4rZ-CgGkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1**<br>\n",
        "Create an RDD called favorites. <br>\n",
        "It should contain 10 of your favorite things. "
      ],
      "metadata": {
        "id": "FUcZKVKKInxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment\n"
      ],
      "metadata": {
        "id": "qOO-QSrGI3cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**count()**"
      ],
      "metadata": {
        "id": "9Lw1cbuGIAUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = words.count()\n",
        "print (\"Number of elements in RDD -> %i\" % (counts))"
      ],
      "metadata": {
        "id": "6iElYitOH_uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2**<br>\n",
        "Count the number of elements in the RDD you created called favorites"
      ],
      "metadata": {
        "id": "JnXrC65wJEbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 2"
      ],
      "metadata": {
        "id": "6OesifzaJR_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collect()**<br>\n",
        "All the elements in the RDD are returned"
      ],
      "metadata": {
        "id": "omopEW4eJWAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coll = words.collect()\n",
        "print (\"Elements in RDD -> %s\" % (coll))"
      ],
      "metadata": {
        "id": "j4KInDmpJYI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3**<br>\n",
        "Collect the elements in favorites"
      ],
      "metadata": {
        "id": "GjMNmzCAJj7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 3"
      ],
      "metadata": {
        "id": "093gD6mEYqWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Y73tbtvVYpIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lambda functions**<br>\n",
        "A lambda function is a small anonymous function.<br>\n",
        "\n",
        "A lambda function can take any number of arguments, but can only have one expression.\n"
      ],
      "metadata": {
        "id": "aUj9lPFtWwyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax for lambda functions:<br>\n",
        ">lambda arguments : expression"
      ],
      "metadata": {
        "id": "M06lqGDZW9kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a is the input to lambda, \n",
        "#returnOfFunction is the input+5\n",
        "returnOfFunction=lambda a: a+5\n",
        "#print the return of the function when a=3\n",
        "print(returnOfFunction(3))"
      ],
      "metadata": {
        "id": "_RqGwURWW6u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a function with more than one input\n",
        "returnOfFunction=lambda a,b: a+b+100\n",
        "print(returnOfFunction(3,4))"
      ],
      "metadata": {
        "id": "4QabfqTWYPq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lambda Assignment**<br>\n",
        "Write a lambda function that adds three input values<br>\n"
      ],
      "metadata": {
        "id": "_hqY6RWoXaLL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ylt5vKaaXeEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "returnOfFunction=lambda a,b,c: a+b+c\n",
        "print(returnOfFunction(3,4,5))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q8P1mZE8X0ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The power of lambda is better shown when you use them as an anonymous function inside another function."
      ],
      "metadata": {
        "id": "i9py9V75Yu01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. define a function using a lambda function"
      ],
      "metadata": {
        "id": "HvbMu8fDficu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def afunction(n):\n",
        "  return lambda a:a*n"
      ],
      "metadata": {
        "id": "hPUVrCPcfHuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "assign a variable to the output of the function<br>\n",
        "(the \"n\" value)"
      ],
      "metadata": {
        "id": "qzNvRrwzfmPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multiplier=afunction(4)"
      ],
      "metadata": {
        "id": "t8W1PbN9gLbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now assign a value to \"a\""
      ],
      "metadata": {
        "id": "Pr0NfNtZgTmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(multiplier(3))"
      ],
      "metadata": {
        "id": "29MeApA8fR1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment:**\n",
        "Create a function that uses a lambda function"
      ],
      "metadata": {
        "id": "eizuv67xg0pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment"
      ],
      "metadata": {
        "id": "MnjaS208g7PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-wKRMzq-fFEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(f)**\n",
        "A filter returns the elements that meet the requested condition"
      ],
      "metadata": {
        "id": "dfHq9T7UBkGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the words dataset is searched for those elements containing 'spark'"
      ],
      "metadata": {
        "id": "sBOCWlEcBm5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words RDD:<br>\n",
        ">[\"scala\", <br>\n",
        "   \"java\",<br> \n",
        "   \"hadoop\", <br>\n",
        "   \"spark\", <br>\n",
        "   \"akka\",<br>\n",
        "   \"spark vs hadoop\", <br>\n",
        "   \"pyspark\",<br>\n",
        "   \"pyspark and spark\"]<br>"
      ],
      "metadata": {
        "id": "pEs3GTsdBpAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_filter = words.filter(lambda x: 'spark' in x)\n",
        "filtered = words_filter.collect()\n",
        "print (\"Fitered RDD -> %s\" % (filtered))"
      ],
      "metadata": {
        "id": "KUSK1-ndBtq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter(f)**\n",
        "A filter returns the elements that meet the requested condition"
      ],
      "metadata": {
        "id": "Jqg1Z5-GDAWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below, the words RDD is searched for those elements containing 'spark'"
      ],
      "metadata": {
        "id": "6T3eB9qqDELo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words dataset:<br>\n",
        ">[\"scala\", <br>\n",
        "   \"java\",<br> \n",
        "   \"hadoop\", <br>\n",
        "   \"spark\", <br>\n",
        "   \"akka\",<br>\n",
        "   \"spark vs hadoop\", <br>\n",
        "   \"pyspark\",<br>\n",
        "   \"pyspark and spark\"]<br>"
      ],
      "metadata": {
        "id": "1m6T9Qd7C7m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_filter = words.filter(lambda x: 'spark' in x)\n",
        "filtered = words_filter.collect()\n",
        "print (\"Filtered RDD -> %s\" % (filtered))"
      ],
      "metadata": {
        "id": "daHqpO02DNgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 4**:<br>\n",
        "Find all the elements in words that contain the letter 'k'"
      ],
      "metadata": {
        "id": "tixnhP5ADV_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 4"
      ],
      "metadata": {
        "id": "h4yWBxFdDWZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduce()**<br>\n",
        "After performing the specified commutative and associative binary operation, the element in the RDD is returned<br>\n",
        "\n",
        "For example: <br>\n",
        "reduce(lambda x, y : x + y, [1,2,3,4,5]) ==> (((1+2)+3)+4)+5)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRznsUXFD5NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "\n",
        "listRdd = spark.sparkContext.parallelize([-1,33,3,4,5,3,2])\n",
        "print(\"output min using binary : \", listRdd.reduce(min))\n",
        "print(\"output add using binary : \", listRdd.reduce(add))\n",
        "\n",
        "print(\"output max using binary : \", listRdd.max())\n"
      ],
      "metadata": {
        "id": "rmax_VLUD-xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mMgpme2ho00T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataFrames**\n",
        "RDD offers low-level control over data, Dataset and DataFrame APIs bring structure and high-level abstractions. Keep in mind that transformations from an RDD to a Dataset or DataFrame are easy to execute"
      ],
      "metadata": {
        "id": "hwEUdRvKPNso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to use Datasets**<br>\n",
        "Use Datasets in situations where:<br>\n",
        "\n",
        "Data requires a structure. <br><br>\n",
        "DataFrames infer a schema on structured and semi-structured data.<br><br>\n",
        "Transformations are high-level. If your data requires high-level processing, columnar functions, and SQL queries, use Datasets and DataFrames.<br><br>\n",
        "A high degree of type safety is necessary. Compile-time type-safety takes full advantage of the speed of development and efficiency."
      ],
      "metadata": {
        "id": "FDxg11BpQXjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "df.select(\"Name\").show()"
      ],
      "metadata": {
        "id": "xhH6wWimOiOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "DXL4hfwLJ9K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create and use a function on the dataset**"
      ],
      "metadata": {
        "id": "KVlVmp7sCOrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(df,col):\n",
        "  df.select(col).show()"
      ],
      "metadata": {
        "id": "EYPRnfv-PLCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(df,\"Name\")"
      ],
      "metadata": {
        "id": "tOv4Y3eOR-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(df,\"Seqno\")"
      ],
      "metadata": {
        "id": "MUeE8zZVR7Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 5**<br>\n",
        "Use the given dataset<br>\n",
        "Show the dataset<br>\n",
        "Show each col individually<br>\n",
        "Count the number of rows<br>"
      ],
      "metadata": {
        "id": "Hm0VM938p3__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 5\n",
        "columns = [\"Race_event\",\"Winner\"]\n",
        "data_assignment4 = [\n",
        "    (\"100yd\", \"baby smith\"),\n",
        "    (\"quarter Mile\", \"bonny jones\"),\n",
        "    (\"half mile\", \"cutie topper\"),\n",
        "    (\"mile\", \"legs mccarty\"),\n",
        "    (\"two mile\", \"speed hari\"),\n",
        "    (\"five mile\", \"betsy boop\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "p3T7SLQmqKko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**foreach(f)**\n",
        "Returns only those elements which meet the condition of the function inside foreach. In the following example, we call a print function in foreach, which prints all the elements in the RDD.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZB2vpB4JrOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this instance we use the accumulator function to gather values from the dataset<br><br>\n",
        "An accumulator is created from an initial value v by calling SparkContext.accumulator(v). <br>\n",
        "\n",
        "Tasks running on the cluster can then add to it using the add method or the += operator (in Scala and Python). However, they cannot read its value."
      ],
      "metadata": {
        "id": "Ut22aCZqCb2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accumulators are for ints and floats. To do other values, use the AccumulatorParam."
      ],
      "metadata": {
        "id": "eowx5TggDG2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def addup(df):\n",
        "  accum=spark.sparkContext.accumulator(0)\n",
        "  df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "  print (accum.value)"
      ],
      "metadata": {
        "id": "uNf6z16-TThO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addup(df)"
      ],
      "metadata": {
        "id": "MvU56W48TcWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example below we comment out the accumulator(0) function. <br>\n",
        "Note what happens"
      ],
      "metadata": {
        "id": "-M_XMNx_DbLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def addup(df):\n",
        "  #accum=spark.sparkContext.accumulator(0)\n",
        "  df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "  print(accum.value)"
      ],
      "metadata": {
        "id": "F-Li-wlZT9m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addup(df)"
      ],
      "metadata": {
        "id": "fvZkPMeZUBD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using foreach**<br>\n",
        "foreach() applied on PySpark DataFrame,executes a function specified in for each element of DataFrame."
      ],
      "metadata": {
        "id": "Sxn8Cgn_EFPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# foreach() with accumulator Example\n",
        "accum=spark.sparkContext.accumulator(0)\n",
        "df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
        "print(accum.value) #Accessed by driver"
      ],
      "metadata": {
        "id": "5XDibKvlRPGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 6**<br>\n",
        "Use the given dataset<br>\n",
        "Sum the appropriate columns"
      ],
      "metadata": {
        "id": "Q0bFibDls5Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 6\n",
        "columns = [\"candy\", \"sales San Jose\",\"sales for California\",\"sales for US\" ]\n",
        "data_assignment6 = [\n",
        "    (\"snickers\",\"34\",\"124\",\"564\"),\n",
        "    (\"goodbar\",\"23\",\"445\",\"876\"),\n",
        "    (\"twix\",\"12\",\"765\",\"1234\"),\n",
        "    (\"m&m\",\"365\",\"987\",\"121234\"),\n",
        "    (\"mars\",\"56\",\"87\",\"234657\"),\n",
        "    (\"red hots\",\"7\",\"67\",\"989877\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "kjsTH_lNtODg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max()**<br>\n",
        "Find the maximum for a column in a dataFrame"
      ],
      "metadata": {
        "id": "XOFh6OCQGYS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max"
      ],
      "metadata": {
        "id": "q4HCx8qIGa4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: that the type of data must be the same for the entire column.** <br>\n",
        "If we make the first row (44,\"a\") and the other rows remain the same, we will get an error. "
      ],
      "metadata": {
        "id": "grxWnD7-Rit9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Number\",\"letter\"]\n",
        "dataNum=[(44.0,\"a\"),(5.0,\"z\"),(7.3,\"c\"),(5.5,\"x\")]\n",
        "dNum=spark.createDataFrame(data=dataNum,schema=columns)"
      ],
      "metadata": {
        "id": "WtqPmiUoGdZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dNum.show()"
      ],
      "metadata": {
        "id": "nP6XPgrLI6Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxValue = dNum.agg(max(\"Number\")).collect()[0][0]\n",
        "print(\"maxValue: \",maxValue)"
      ],
      "metadata": {
        "id": "oVWI9Gf5GzNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notice: when the number values are entered as strings, the max is based on alphabetical order.**"
      ],
      "metadata": {
        "id": "iE7rLe5iSZPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Number\",\"letter\"]\n",
        "dataNum=[(\"44.0\",\"a\"),(\"5.0\",\"z\"),(\"7.3\",\"c\"),(\"5.5\",\"x\")]\n",
        "dNum=spark.createDataFrame(data=dataNum,schema=columns)\n",
        "maxValue = dNum.agg(max(\"Number\")).collect()[0][0]\n",
        "print(\"maxValue: \",maxValue)"
      ],
      "metadata": {
        "id": "FxwZ1mQwR57k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can cast values to different types using the cast function. "
      ],
      "metadata": {
        "id": "jR6MKns2Tlfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Number\",\"letter\"]\n",
        "dataNum=[(\"44.0\",\"a\"),(\"5.0\",\"z\"),(\"7.3\",\"c\"),(\"5.5\",\"x\")]\n",
        "dNum=spark.createDataFrame(data=dataNum,schema=columns)\n",
        "#Cast the values to numbers\n",
        "dNum=dNum.select(col(\"Number\").cast('float').alias(\"Number\"))\n",
        "maxValue = dNum.agg(max(\"Number\")).collect()[0][0]\n",
        "print(\"maxValue: \",maxValue)"
      ],
      "metadata": {
        "id": "8393DC4GSk6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 7**<br>\n",
        "Use the given dataset, <br>\n",
        "Find the maxvalue for all appropriate colmuns<br>\n",
        "You will need to cast to values to ints if you want the largest number. "
      ],
      "metadata": {
        "id": "c3XW6Exu3nvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 7\n",
        "columns = [\"candy\", \"sales San Jose\",\"sales for California\",\"sales for US\" ]\n",
        "data_assignment7 = [\n",
        "    (\"snickers\",\"34\",\"124\",\"564\"),\n",
        "    (\"goodbar\",\"23\",\"445\",\"876\"),\n",
        "    (\"twix\",\"12\",\"765\",\"1234\"),\n",
        "    (\"m&m\",\"365\",\"987\",\"121234\"),\n",
        "    (\"mars\",\"56\",\"87\",\"234657\"),\n",
        "    (\"red hots\",\"7\",\"67\",\"989877\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "7KqOUm8D3-le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter() on a a DataFrame**"
      ],
      "metadata": {
        "id": "EGtHzVseEwMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create the dataframe\n",
        "data12 = [[1, \"sravan\", \"company 1\"],\n",
        "        [2, \"ojaswi\", \"company 1\"], \n",
        "        [3, \"rohith\", \"company 2\"],\n",
        "        [4, \"sridevi\", \"company 1\"],\n",
        "        [1, \"sravan\", \"company 1\"],\n",
        "        [4, \"sridevi\", \"company 1\"]]\n",
        "columns = ['ID', 'NAME', 'Company']\n",
        "dataframe12=spark.createDataFrame(data12, columns)\n",
        "dataframe12.show()"
      ],
      "metadata": {
        "id": "0NrFxh29E1Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using \"where\" to filter"
      ],
      "metadata": {
        "id": "kBQi5kMmF5kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select dataframe values where ID less than 3\n",
        "# OR name is sridevi \n",
        "# AND company 1\n",
        "dataframe12.where((dataframe12.ID < 3) | (\n",
        "\t(dataframe12.NAME == 'sridevi') &\n",
        "(dataframe12.Company == 'company 1'))).show()\n"
      ],
      "metadata": {
        "id": "egJKID9uF47K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select dataframe where ID less than 3\n",
        "dataframe12.filter(dataframe12.ID < 3).show()"
      ],
      "metadata": {
        "id": "ZvAVSnaeFPUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select dataframe where ID less than 3\n",
        "#OR name is sridevi\n",
        "dataframe12.filter((dataframe12.ID < 3) |\n",
        "\t\t\t\t(dataframe12.NAME == 'sridevi')).show()\n"
      ],
      "metadata": {
        "id": "p2xVzs4IFZpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select dataframe where ID less than e\n",
        "# OR name is sridevi \n",
        "# AND company 1\n",
        "dataframe12.filter((dataframe12.ID < 3) | (\n",
        "\t(dataframe12.NAME == 'sridevi') &\n",
        "(dataframe12.Company == 'company 1'))).show()\n"
      ],
      "metadata": {
        "id": "_bnCZBe3Fmg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 8**<br>\n",
        "Use the given dataset<br>\n",
        "Create at least 3 filters for the dataset"
      ],
      "metadata": {
        "id": "GoJsEtEOJbhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 8\n",
        "columns = [\"candy\", \"sales San Jose\",\"sales for California\",\"sales for US\" ]\n",
        "data_assignment7 = [\n",
        "    (\"snickers\",\"34\",\"124\",\"564\"),\n",
        "    (\"goodbar\",\"23\",\"445\",\"876\"),\n",
        "    (\"twix\",\"12\",\"765\",\"1234\"),\n",
        "    (\"m&m\",\"365\",\"987\",\"121234\"),\n",
        "    (\"mars\",\"56\",\"87\",\"234657\"),\n",
        "    (\"red hots\",\"7\",\"67\",\"989877\")\n",
        "    ]"
      ],
      "metadata": {
        "id": "1CAEb5_UJTvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Partitioning?"
      ],
      "metadata": {
        "id": "hX3JNlT8Q-HK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page(\"Ideal-Partitioning\")"
      ],
      "metadata": {
        "id": "m4LpE3RfVXy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark/Pyspark partitioning is a way to split the data into multiple partitions so that you can execute transformations on multiple partitions in parallel which allows completing the job faster. <br>\n",
        "\n",
        "You can also write partitioned data into a file system (multiple sub-directories) for faster reads by downstream systems."
      ],
      "metadata": {
        "id": "9PsaXJxqUivm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spark/Pyspark creates partitions that are equal to the number of CPU cores in the machine**.<br>\n",
        "Data of each partition resides in a single machine.<br>\n",
        "Spark/Pyspark creates a task for each partition.\n",
        "Spark Shuffle operations move the data from one partition to other partitions.<br>\n",
        "**Partitioning is an expensive operation as it creates a data shuffle** (Data could move between the nodes)<br><br>\n",
        "By default, Dataframe shuffle operations create 200 partitions."
      ],
      "metadata": {
        "id": "7JK9BYBRUoXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**map()** transforms the RDD by applying the lambda function to every element in the dataframe. "
      ],
      "metadata": {
        "id": "546rLf3bFjjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "9IX4bl2kFxht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this instance we are mapping  a second value (23) to each element"
      ],
      "metadata": {
        "id": "CzPXbH8J5Zqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_map = rdd.map(lambda x: (x, 23))\n",
        "mapping = words_map.collect()\n",
        "print (\"Key value pair -> %s\" % (mapping))"
      ],
      "metadata": {
        "id": "ya2g9upjF5GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for element in words_map.collect():\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "Az9ZEpVG4nyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"partitions=\",rdd.getNumPartitions())\n",
        "print(\"count=\",words_map.count())\n",
        "print(words_map.collect())"
      ],
      "metadata": {
        "id": "yBoaS9beN_iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 9**<br>\n",
        "Add another element to words_map"
      ],
      "metadata": {
        "id": "FMXBMvIl6BOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 9"
      ],
      "metadata": {
        "id": "Itr-4wXv6J5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "words_map2 = words_map.map(lambda x: (x, \"windy\"))\n",
        "mapping = words_map2.collect()\n",
        "print (\"Key value pair -> %s\" % (mapping))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GfGKzQA16ifJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**reduce()**<br>\n",
        "After performing the specified commutative and associative binary operation, the element in the RDD is returned<br>\n",
        "\n",
        "For example: <br>\n",
        "reduce(lambda x, y : x + y, [1,2,3,4,5]) ==> (((1+2)+3)+4)+5)\n",
        "\n"
      ],
      "metadata": {
        "id": "esuEQh7HRgAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "\n",
        "listRdd = spark.sparkContext.parallelize([-1,33,3,4,5,3,2])\n",
        "print(\"output min using binary : \", listRdd.reduce(min))\n",
        "print(\"output add using binary : \", listRdd.reduce(add))\n",
        "\n",
        "print(\"output max using binary : \", listRdd.max())\n"
      ],
      "metadata": {
        "id": "A8P5ItSeRhIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 10**<br>\n",
        "Create an RDD with at least 8 elements<br> \n",
        "Then find the min and the sum of the elements"
      ],
      "metadata": {
        "id": "3XtqhzRvR2P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 10"
      ],
      "metadata": {
        "id": "-eERBvp063tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**join**<br>"
      ],
      "metadata": {
        "id": "If0XH5PeR2Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
        "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
        "joined = x.join(y)\n",
        "final = joined.collect()\n",
        "print (\"Join RDD -> %s\" % (final))"
      ],
      "metadata": {
        "id": "N5oyFxpyR5SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cache()**<br>\n",
        "Pyspark cache() method is used to cache the intermediate results of the transformation so that other transformation runs on top of cached will perform faster. Caching the result of the transformation is one of the optimization tricks to improve the performance of the long-running PySpark applications/jobs."
      ],
      "metadata": {
        "id": "NLnlR_XNSOUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost-efficient – Spark computations are very expensive hence reusing the computations are used to save cost.<br>\n",
        "Time-efficient – Reusing repeated computations saves lots of time.<br>\n",
        "Execution time – Saves execution time of the job and we can perform more jobs on the same cluster.<br>\n"
      ],
      "metadata": {
        "id": "YgxRWD7fKpj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ") \n",
        "words.cache() \n",
        "caching = words.persist().is_cached \n",
        "print( \"Words got chached > %s\" % (caching))"
      ],
      "metadata": {
        "id": "3NDzfFEnSOil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**broadcast()**<br>\n",
        "A Broadcast variable has an attribute called value, which stores the data and is used to return a broadcasted value.<br>\n",
        "\n",
        "Broadcast variables are used to save the copy of data across all nodes. This variable is cached on all the machines and not sent on machines with tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "1nhXgdnJSdbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
        "data = words_new.value \n",
        "print (\"Stored data -> %s\" % (data) )\n",
        "elem = words_new.value[2] \n",
        "print (\"Printing a particular element in RDD -> %s\" % (elem))"
      ],
      "metadata": {
        "id": "BLKu_9xnSdr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accumulator()**<br>\n",
        "Sums all values in the RDD"
      ],
      "metadata": {
        "id": "bGtyqe5vS7Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = sc.accumulator(10) \n",
        "def f(x): \n",
        "   global num \n",
        "   num+=x \n",
        "rdd = sc.parallelize([20,30,40,50]) \n",
        "rdd.foreach(f) \n",
        "final = num.value \n",
        "print (\"Accumulated value is -> %i\" % (final))"
      ],
      "metadata": {
        "id": "__BSp36pS80R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GroupBy()"
      ],
      "metadata": {
        "id": "RmsHMDgxJHaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a simple dataset"
      ],
      "metadata": {
        "id": "bkmyjrFKJOq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
        "  ]\n"
      ],
      "metadata": {
        "id": "0MrACiPYJLLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the data schema - the column names"
      ],
      "metadata": {
        "id": "smDfN4kZJU22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]"
      ],
      "metadata": {
        "id": "6TKcabbYJTk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataframe"
      ],
      "metadata": {
        "id": "5t0Gs1Q-Jamn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "ZjOLFMMzJayW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using groupby()**<br>\n",
        "Group the dataframe into three categories: Sales, Finance, Marketing<br>\n",
        "Notice the salary is the sum of the values in each category"
      ],
      "metadata": {
        "id": "oZDKuWEGKDGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
      ],
      "metadata": {
        "id": "ny3_V0whKDRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groupy by number of employees"
      ],
      "metadata": {
        "id": "7VZF8oRHKhjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").count().show()"
      ],
      "metadata": {
        "id": "8BqkMsZ0KhuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").min(\"salary\").show()"
      ],
      "metadata": {
        "id": "DMdMjIKQLS4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").max(\"salary\").show()"
      ],
      "metadata": {
        "id": "opp3m1xYLS6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").mean( \"salary\").show()"
      ],
      "metadata": {
        "id": "OUu1NQRTLa3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groupby on multiple columns<br>\n",
        "Group by department and state"
      ],
      "metadata": {
        "id": "pbXsjBGDLiVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GroupBy on multiple columns\n",
        "df.groupBy(\"department\",\"state\") \\\n",
        "    .sum(\"salary\",\"bonus\") \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "id": "J0b_khOhLk6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Doing multiple aggrates at the same time**"
      ],
      "metadata": {
        "id": "EXrzBZxvL-93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,avg,max\n",
        "df.groupBy(\"department\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
        "         max(\"bonus\").alias(\"max_bonus\") \\\n",
        "     ) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "id": "e3GAS7pFL_Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 11:** <br>\n",
        "Create a dataframe with the following column headings. <br>\n",
        "Create at least 10 rows <br>\n",
        "\n",
        "Name, state, team_name, event_name, race_time1, race_time2, race_time3<br>\n",
        "\n",
        "Use Groupby to group the dataframe"
      ],
      "metadata": {
        "id": "xQPLCCGCSq7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment 11"
      ],
      "metadata": {
        "id": "zVBlenKXMEtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring Spark"
      ],
      "metadata": {
        "id": "_f5gSlkbUrEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Following are some of the most commonly used attributes of SparkConf −\n",
        "\n",
        ">set(key, value) − To set a configuration property.\n",
        "\n",
        ">setMaster(value) − To set the master URL.\n",
        "\n",
        ">setAppName(value) − To set an application name.\n",
        "\n",
        ">get(key, defaultValue=None) − To get a configuration value of a key.\n",
        "\n",
        ">setSparkHome(value) − To set Spark installation path on worker nodes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dhl7lGGVUyPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf"
      ],
      "metadata": {
        "id": "XuP1CWw1VBpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
        "#sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "IEEUPpLLUvVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get(filename)<br>\n",
        "getrootdirectory()"
      ],
      "metadata": {
        "id": "h4Cq_bnWV15d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "finddistance = \"/content/cloned-repo/airlines.csv\"\n",
        "finddistancename = \"finddistance.R\"\n",
        "sc.addFile(finddistance)\n",
        "print (\"Absolute Path -> %s\" % SparkFiles.get(finddistancename))"
      ],
      "metadata": {
        "id": "J2HtPVm-VcRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Storage Levels**"
      ],
      "metadata": {
        "id": "yP6drY9_V_Os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISK_ONLY = StorageLevel(True, False, False, False, 1)\n",
        "\n",
        "DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n",
        "\n",
        "MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n",
        "\n",
        "MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n",
        "\n",
        "MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\n",
        "\n",
        "MEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\n",
        "\n",
        "MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n",
        "\n",
        "MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n",
        "\n",
        "MEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\n",
        "\n",
        "MEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\n",
        "\n",
        "OFF_HEAP = StorageLevel(True, True, True, False, 1)"
      ],
      "metadata": {
        "id": "72LlRhNlWByx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "rdd1 = sc.parallelize([1,2])\n",
        "rdd1.persist( pyspark.StorageLevel.MEMORY_AND_DISK_2 )\n",
        "rdd1.getStorageLevel()\n",
        "print(rdd1.getStorageLevel())\n"
      ],
      "metadata": {
        "id": "7dWB1h8kWBSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}