{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNYzOPpQEj0BOvbkPicT/4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySpark_2_Data_Manipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Manipulation<br>\n",
        "**This notebook demonstrates a few methods of data manipulation using PySpark**"
      ],
      "metadata": {
        "id": "9BfasjaJNEQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "#%cd cloned-repo\n",
        "#!ls"
      ],
      "metadata": {
        "id": "CgduyP92Uv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"/content/cloned-repo/\"+str(num)+ \".png\" , width=640)"
      ],
      "metadata": {
        "id": "fS83pNJuVBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install PySpark for Google CoLabs**"
      ],
      "metadata": {
        "id": "NN4D2UXLNx7t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7wQrL7jJVKM"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libaries and start a SparkSession**"
      ],
      "metadata": {
        "id": "Wxd4cTVSN3M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "#getOrCreate gets or creates a session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "BYmRHz5mJeWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "4fmKVmnaJipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print(\"If no error - everything is working\")\n"
      ],
      "metadata": {
        "id": "o3WlhnO8JmLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "y3LaVsmtIcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools we need to connect to the Spark server, load our data,\n",
        "clean it and prepare it**"
      ],
      "metadata": {
        "id": "uefIbuRZOCEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "metadata": {
        "id": "d8swmeMGJsUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zPEx-WZvJy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StringIndexer<br>\n",
        "**Use stringIndexer when you want a column identified as catagorical data.** "
      ],
      "metadata": {
        "id": "WwdwZja686ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StringIndexer maps a string column to an index column that will be treated as a categorical column by spark. The indices start with 0 and are ordered by label frequencies. If it is a numerical column, the column will be cast as a string column and then indexed by StringIndexer.\n",
        "\n",
        "There are three steps to implement the StringIndexer\n",
        "\n",
        "- Build the StringIndexer model: specify the input column and output column names.\n",
        "- Learn the StringIndexer model: fit the model with your data.\n",
        "- Execute the indexing: call the transform function to execute the indexing process."
      ],
      "metadata": {
        "id": "lWuvYjaeLjtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StringIndexer maps a string column to a index column that will be treated as a categorical column by spark. The indices start with 0 and are ordered by label frequencies. If it is a numerical column, the column will be cast as a string column and then indexed by StringIndexer.\n",
        "\n",
        "There are three steps to implement the StringIndexer\n",
        "\n",
        "Build the StringIndexer model: specify the input column and output column names.\n",
        "Learn the StringIndexer model: fit the model with your data.\n",
        "Execute the indexing: call the transform function to execute the indexing process."
      ],
      "metadata": {
        "id": "valBAeQCbgkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a dataframe**"
      ],
      "metadata": {
        "id": "McG4cDjLOS2T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2gSHi0da3pF"
      },
      "outputs": [],
      "source": [
        "\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
        "        'y1': [1, 0, 1, 0, 0, 1],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
        "    })\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert catagorical data to numerical data**"
      ],
      "metadata": {
        "id": "5bh_TLtaOZjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# build indexer\n",
        "string_indexer = StringIndexer(inputCol='x1', outputCol='indexed_x1')\n",
        "\n",
        "# learn the model\n",
        "string_indexer_model = string_indexer.fit(df)\n",
        "\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model.transform(df)\n",
        "\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "metadata": {
        "id": "cilw1neYKGP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notice in the indexed_x1 column the value 'b' is the most numerous so it gets the value 0**<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "HjBX3uz49ggr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**<br>\n",
        "Column x2 contains catagorical data. <br>\n",
        "Convert it to data the ML model can use. "
      ],
      "metadata": {
        "id": "_es_es6992BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment\n"
      ],
      "metadata": {
        "id": "hkDWR0R8-CYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "# build indexer\n",
        "string_indexer = StringIndexer(inputCol='x2', outputCol='indexed_x2')\n",
        "\n",
        "# learn the model\n",
        "string_indexer_model = string_indexer.fit(df_stringindexer)\n",
        "\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model.transform(df_stringindexer)\n",
        "\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ti1sWuVv9Nmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Map**<br>\n",
        "(map()) is defined as the RDD transformation that is used to apply the transformation function (Lambda) on every element of Resilient Distributed Datasets(RDD) or DataFrame and further returns a new Resilient Distributed Dataset(RDD)."
      ],
      "metadata": {
        "id": "4_9AfHdBO4aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_map = df.rdd.map(lambda x: (x['x1'], x['x2']))\n",
        "df_map.take(5)"
      ],
      "metadata": {
        "id": "adLH1azUccX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_map = df.rdd.map(lambda x: (x['x1'], x['x3']))\n",
        "df_mapvalues =df_map.mapValues(lambda x: [x, x * 3])\n",
        "df_mapvalues.take(5)"
      ],
      "metadata": {
        "id": "--eGrsHRnS1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD to DataFrame**<br>\n",
        "To convert an RDD to a DataFrame, we can use the SparkSession.createDataFrame() function. \n",
        "\n"
      ],
      "metadata": {
        "id": "gnR7APU0oB7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.parallelize (\n",
        "   [\"software\",\n",
        "   \"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ")"
      ],
      "metadata": {
        "id": "VGD91_A3oInX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".take(num) Returns the first num rows as a list of Row."
      ],
      "metadata": {
        "id": "4oQHp5DwxCMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words.take(6)"
      ],
      "metadata": {
        "id": "CBuSzVPNoZ_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = words.map(lambda x: x.split(',')).filter(lambda x: x == 'software').collect()\n",
        "header = 'type'\n",
        "header"
      ],
      "metadata": {
        "id": "hvfFRF_9okZ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}