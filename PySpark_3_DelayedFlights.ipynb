{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySpark_3_DelayedFlights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing data for Machine Learning Models\n",
        "This notebook uses a dataset of airplane flights. The dataset has 5.8million rows.<br>\n",
        "\n",
        "This notebook demonstrates some of the commands that are used to prepare the data for a machine learning model. "
      ],
      "metadata": {
        "id": "zL4ceRlpTtza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook requires a large dataset be loaded onto your Google Drive because it is too large for the github cloned repo. "
      ],
      "metadata": {
        "id": "b2I7IfDA4x8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Go to this [kaggle link](https://www.kaggle.com/code/miquar/explore-flights-csv-airports-csv-airlines-csv/data?select=flights.csv)<br>\n",
        "\n",
        "2. Download the data by clicking on the Download button in the top right corner. \n",
        "\n",
        "3. This will download to your local computer a zip folder called archive.zip\n",
        "\n",
        "4. Open the folder by clicking on the archive folder\n",
        "\n",
        "5. Go to your Google drive and upload flights.csv by doing the following: \n",
        ">a. In the upper left corner click on \"New\"<br>\n",
        "b. Click on Upload file<br>\n",
        "c. On your local machine find the archive folder that you opened<br>\n",
        "d. Select the file called flights.csv<br>\n",
        " This is a large file (565MB) so it will take a few minutes to upload. \n",
        "\n",
        "Once you see the file in your Google Drive, you are ready to start working with this notebook.\n"
      ],
      "metadata": {
        "id": "hhMRj-BH6q20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount your Google Drive to this notebook**"
      ],
      "metadata": {
        "id": "iuZ2yq36Cw9A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ettu8VG7-KAI"
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed Computing<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "2i0Jm7Kj81iU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WcC7uUz7S4u"
      },
      "source": [
        "https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3\n",
        "\n",
        "\n",
        "https://medium.com/grabngoinfo/install-pyspark-3-on-google-colab-the-easy-way-577ec4a2bcd8\n",
        "\n",
        "Check for the latest version of Spark at https://pypi.org/project/pyspark/#history\n",
        "\n",
        "https://www.transtats.bts.gov/OT_Delay/OT_DelayCause1.asp\n",
        "\n",
        "\n",
        "https://grabngoinfo.com/install-pyspark-3-on-google-colab-the-easy-way/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cagBRT/PySpark.git"
      ],
      "metadata": {
        "id": "vn0ujiMgQXc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "gkVZEiasOkh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an Entry Point to PySpark\n",
        "<br>\n",
        "SparkSession starts the PySpark Session. \n",
        "<br>\n",
        "This begins the ability to program with RDD (Resilient Data Distribution), DataFrame, and DataSet"
      ],
      "metadata": {
        "id": "cq9uwbI57gzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "#getOrCreate gets or creates a session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "id": "vvON8-UoOqQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "7ZLp5GP1OtxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD5KzNGA7RZH"
      },
      "source": [
        "import os\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "#import findspark\n",
        "#findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "print(\"If no error - everything is working\")\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "# Set up constants\n",
        "CSV_2007= \"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\" \n",
        "CSV_2008= \"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\"\n",
        "APP_NAME = \"Flight Delays\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141109\n",
        "TRAINING_DATA_RATIO = 0.7\n",
        "RF_NUM_TREES = 8\n",
        "RF_MAX_DEPTH = 4\n",
        "RF_NUM_BINS = 32\n"
      ],
      "metadata": {
        "id": "vSyTP-DgK4lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to mount the Gdrive because the dataset is in three files and one of them is quite large. <bR>\n",
        "The cloned repo contains two of the files:<Br>\n",
        "\"/content/cloned-repo/airports.csv\"<br>\n",
        "\"/content/cloned-repo/airlines.csv\"<br>\n",
        "<br>\n",
        "The third file needs to be loaded onto your GDrive and then the program can read it from there. \n"
      ],
      "metadata": {
        "id": "y8DzAiKP3w7J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_SqXaIST-eQ"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/PySpark.git cloned-repo\n",
        "#%cd cloned-repo\n",
        "#!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k67iOVTXEfEg"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1RFC0UzFEX5"
      },
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up constants\n",
        "FLIGHTS= \"/gdrive/MyDrive/flights.csv\" \n",
        "AIRPORTS= \"/content/cloned-repo/airports.csv\"\n",
        "AIRLINES= \"/content/cloned-repo/airlines.csv\"\n",
        "APP_NAME = \"Flight Delays\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141109\n",
        "TRAINING_DATA_RATIO = 0.7\n",
        "RF_NUM_TREES = 8\n",
        "RF_MAX_DEPTH = 4\n",
        "RF_NUM_BINS = 32"
      ],
      "metadata": {
        "id": "IsOmfEz_FhRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Spark session has to be started. "
      ],
      "metadata": {
        "id": "vuvH93ewDCaB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0V8_n8LFJAb"
      },
      "source": [
        "# Connect to the Spark server\n",
        "spark = SparkSession.builder.appName(APP_NAME).master(FLIGHTS).getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is 5.8 milllion rows of data. <br>\n",
        "It will take a minute to load it into the Spark session"
      ],
      "metadata": {
        "id": "VyOpm6jVil0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "flights_df = spark.read.options(header=\"true\",inferschema = \"true\").csv(FLIGHTS)"
      ],
      "metadata": {
        "id": "CfOlY85HibQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next command can take a while to execute, be patient or skip it. "
      ],
      "metadata": {
        "id": "07mkZWHqEcbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights_df.describe().show()"
      ],
      "metadata": {
        "id": "mpAQ-bv4D9iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is 5.8million rows by 31 columns. <br>\n",
        "If your dataset is not 5819079 rows by 31 columns, you may not have uploaded the data properly. "
      ],
      "metadata": {
        "id": "kXjiWSnnidao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see our dataset is unbalanced. We have way more \"Not cancelled\" flights than \"cancelled\" flights."
      ],
      "metadata": {
        "id": "NIK9PgLWF2J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datacount=flights_df.groupBy(\"CANCELLED\").count()\n",
        "datacount.show()"
      ],
      "metadata": {
        "id": "jse-2FMxFjhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-raO-RCFzje"
      },
      "source": [
        "print(f\"The shape is {flights_df.count():d} rows by {len(flights_df.columns):d} columns.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has a lot of null values (missing values)<br>\n",
        "30 million missing values"
      ],
      "metadata": {
        "id": "tRcV9PGtizNm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj-kmimfGHoS"
      },
      "source": [
        "null_counts = flights_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)for c in flights_df.columns]).toPandas().to_dict(orient='records')\n",
        "print(f\"We have {sum(null_counts[0].values()):d} null values in this dataset.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list of columns and data types shows there are string value columns. If we are going to use this data we need to either change the strings to values the model can ingest or remove the columns."
      ],
      "metadata": {
        "id": "YDZS1PeJGIJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights_df.printSchema()"
      ],
      "metadata": {
        "id": "Ae-akTZGEXqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7nogRdaCD7u"
      },
      "source": [
        "flights_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's drop the CANCELLATION_REASON column"
      ],
      "metadata": {
        "id": "kqqZjPceGYfx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRGyVWc-HnlV"
      },
      "source": [
        "flights_df = flights_df.drop(flights_df.CANCELLATION_REASON)\n",
        "flights_df = flights_df.na.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gykHxAmtH2tS"
      },
      "source": [
        "flights_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select from the \"CANCELLED\" column only unique values. <br>\n",
        "We select to keep only the rows where the flight was not cancelled<br>\n",
        "\n",
        "1= flight cancelled<br>\n",
        "\n",
        "0=flight not cancelled"
      ],
      "metadata": {
        "id": "Jkul-AS_UqBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the model to predict if a flight will be cancelled. "
      ],
      "metadata": {
        "id": "-CVIXJ_V7f-h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq1sdujBH566"
      },
      "source": [
        "#flights_df.select('CANCELLED').distinct().rdd.map(lambda r: r[0]).collect()\n",
        "#flights_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our features for the model will be only those inputs that will halp make a prediction. "
      ],
      "metadata": {
        "id": "Vioo1vA9GxCX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeTswmtlID66"
      },
      "source": [
        "feature_cols = ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_TIME', 'ARRIVAL_DELAY', 'FLIGHT_NUMBER', 'DISTANCE', 'DIVERTED']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols"
      ],
      "metadata": {
        "id": "s7NwkF0XG3Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VectorAssembler = A feature transformer that merges multiple columns into a vector column.<br>\n",
        "Next we add the features column to the dataset"
      ],
      "metadata": {
        "id": "FAta7RQDYLe0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K52xVd6EIb4J"
      },
      "source": [
        "flights_df = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").transform(flights_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datacount=flights_df.groupBy(\"CANCELLED\").count()\n",
        "datacount.show()"
      ],
      "metadata": {
        "id": "7ecjIsTEHEI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The features column is at the end of the columns list"
      ],
      "metadata": {
        "id": "yu2z6bbjKHMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights_df.show()"
      ],
      "metadata": {
        "id": "Gf7wU9MEG7Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a subset of the data**"
      ],
      "metadata": {
        "id": "vgbMXhONFUa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a subset of the dataset, selecting only those flights that were cancelled"
      ],
      "metadata": {
        "id": "5Ado_7mY8Dt-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT9K47_UIfF8"
      },
      "source": [
        "flights_cancelled.select(\"Cancelled\", \"features\").show(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting columns will get us a subset of the dataset"
      ],
      "metadata": {
        "id": "g9VXxj9mFJVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights_subset.select(\"Origin_airport\",\"Destination_airport\",\"Airline\",\"Diverted\").show(15)"
      ],
      "metadata": {
        "id": "J-51fxZcFEz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=flights_df.select(\"Cancelled\", \"features\").show(5)"
      ],
      "metadata": {
        "id": "jZjayyEMKRCl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}